{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import models, layers, optimizers\n",
    "from keras.applications import VGG16\n",
    "from keras import backend as K\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "import gc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "# matplotlib.rcParams['figure.figsize'] = [10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOV_VALS = [30, 40, 50, 60, 70]\n",
    "\n",
    "PARENT_DIR = '/media/johnkoo/data/stanford-dogs-dataset/'\n",
    "val_dir = os.path.join(PARENT_DIR, 'val')\n",
    "test_dir = os.path.join(PARENT_DIR, 'test')\n",
    "\n",
    "NUM_CLASSES = 120\n",
    "\n",
    "INPUT_SHAPE = (128, 128, 3)\n",
    "\n",
    "BATCH_SIZE = 160\n",
    "\n",
    "RUNS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(val_dir)\n",
    "val_x = np.load('val-images.npy') / 255.\n",
    "val_y = np.load('val-labels.npy')\n",
    "\n",
    "os.chdir(test_dir)\n",
    "test_x = np.load('test-images.npy') / 255.\n",
    "test_y = np.load('test-labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSET_SIZE = 120\n",
    "\n",
    "dogs = np.unique(val_y)\n",
    "dogs_subset = np.random.choice(dogs, SUBSET_SIZE, replace=False)\n",
    "\n",
    "val_ind = np.where(np.isin(val_y, dogs_subset))[0]\n",
    "test_ind = np.where(np.isin(test_y, dogs_subset))[0]\n",
    "\n",
    "val_x = val_x[val_ind, :, :, :]\n",
    "test_x = test_x[test_ind, :, :, :]\n",
    "\n",
    "val_y = val_y[val_ind]\n",
    "test_y = test_y[test_ind]\n",
    "\n",
    "y_dict = dict([(y, x) for x, y in enumerate(sorted(set(val_y)))])\n",
    "\n",
    "y_val = np.asarray([y_dict[x] for x in val_y])\n",
    "y_test = np.asarray([y_dict[x] for x in test_y])\n",
    "\n",
    "y_val = np_utils.to_categorical(y_val)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=INPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 36s 6ms/step - loss: 4.7997 - acc: 0.0130 - val_loss: 4.7873 - val_acc: 0.0127\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.5639 - acc: 0.0418 - val_loss: 4.8468 - val_acc: 0.0177\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 3.9824 - acc: 0.1000 - val_loss: 4.9214 - val_acc: 0.0213\n",
      "4222/4222 [==============================] - 8s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8112 - acc: 0.0112 - val_loss: 4.7862 - val_acc: 0.0097\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6337 - acc: 0.0372 - val_loss: 4.8225 - val_acc: 0.0157\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 4.1178 - acc: 0.0867 - val_loss: 4.9476 - val_acc: 0.0220\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8146 - acc: 0.0090 - val_loss: 4.7873 - val_acc: 0.0120\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6112 - acc: 0.0367 - val_loss: 4.8411 - val_acc: 0.0157\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.1014 - acc: 0.0818 - val_loss: 5.0113 - val_acc: 0.0187\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8110 - acc: 0.0080 - val_loss: 4.7949 - val_acc: 0.0070\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6345 - acc: 0.0332 - val_loss: 4.8141 - val_acc: 0.0110\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 4.1325 - acc: 0.0838 - val_loss: 4.9456 - val_acc: 0.0143\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.7986 - acc: 0.0132 - val_loss: 4.7861 - val_acc: 0.0130\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.5405 - acc: 0.0482 - val_loss: 4.8528 - val_acc: 0.0147\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.9451 - acc: 0.1015 - val_loss: 4.8939 - val_acc: 0.0263\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8092 - acc: 0.0090 - val_loss: 4.7875 - val_acc: 0.0113\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6380 - acc: 0.0375 - val_loss: 4.8186 - val_acc: 0.0123\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.1469 - acc: 0.0812 - val_loss: 4.9314 - val_acc: 0.0213\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8171 - acc: 0.0102 - val_loss: 4.7979 - val_acc: 0.0110\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6439 - acc: 0.0353 - val_loss: 4.7918 - val_acc: 0.0130\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.1113 - acc: 0.0867 - val_loss: 5.2906 - val_acc: 0.0173\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 3.5185 - acc: 0.1630 - val_loss: 5.2350 - val_acc: 0.0210\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 4.8120 - acc: 0.0122 - val_loss: 4.7963 - val_acc: 0.0103\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 4.6315 - acc: 0.0290 - val_loss: 4.8183 - val_acc: 0.0127\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 4.1695 - acc: 0.0790 - val_loss: 4.8622 - val_acc: 0.0260\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 4.8162 - acc: 0.0092 - val_loss: 4.7822 - val_acc: 0.0120\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 4.6454 - acc: 0.0333 - val_loss: 4.6816 - val_acc: 0.0193\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 4.1728 - acc: 0.0842 - val_loss: 4.5885 - val_acc: 0.0443\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 3.4568 - acc: 0.1795 - val_loss: 4.3790 - val_acc: 0.0640\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 2.6876 - acc: 0.3225 - val_loss: 4.3939 - val_acc: 0.0880\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 1.9375 - acc: 0.4955 - val_loss: 4.3563 - val_acc: 0.1037\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 1.2476 - acc: 0.6593 - val_loss: 5.0337 - val_acc: 0.1077\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 0.7277 - acc: 0.7930 - val_loss: 5.4211 - val_acc: 0.1097\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8254 - acc: 0.0082 - val_loss: 4.7919 - val_acc: 0.0100\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 4.6996 - acc: 0.0230 - val_loss: 4.7437 - val_acc: 0.0150\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.3148 - acc: 0.0592 - val_loss: 4.6194 - val_acc: 0.0337\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.7448 - acc: 0.1323 - val_loss: 4.3992 - val_acc: 0.0580\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 3.1226 - acc: 0.2442 - val_loss: 4.5161 - val_acc: 0.0677\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.4652 - acc: 0.3770 - val_loss: 4.5216 - val_acc: 0.0863\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8164 - acc: 0.0095 - val_loss: 4.7756 - val_acc: 0.0100\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6596 - acc: 0.0323 - val_loss: 4.7037 - val_acc: 0.0173\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 4.2115 - acc: 0.0845 - val_loss: 4.6822 - val_acc: 0.0313\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.4514 - acc: 0.1838 - val_loss: 4.4435 - val_acc: 0.0713\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.6861 - acc: 0.3312 - val_loss: 4.3186 - val_acc: 0.0973\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.8737 - acc: 0.5032 - val_loss: 4.9598 - val_acc: 0.0850\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.3008 - acc: 0.6448 - val_loss: 5.2289 - val_acc: 0.1060\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8147 - acc: 0.0102 - val_loss: 4.7810 - val_acc: 0.0120\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6477 - acc: 0.0333 - val_loss: 4.7168 - val_acc: 0.0207\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.1840 - acc: 0.0807 - val_loss: 4.6862 - val_acc: 0.0357\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.5101 - acc: 0.1747 - val_loss: 4.4277 - val_acc: 0.0573\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.7629 - acc: 0.3212 - val_loss: 4.6364 - val_acc: 0.0810\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.0696 - acc: 0.4595 - val_loss: 4.6300 - val_acc: 0.0903\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8184 - acc: 0.0085 - val_loss: 4.7747 - val_acc: 0.0157\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6802 - acc: 0.0282 - val_loss: 4.7159 - val_acc: 0.0193\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.3140 - acc: 0.0675 - val_loss: 4.6669 - val_acc: 0.0313\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.6144 - acc: 0.1543 - val_loss: 4.5023 - val_acc: 0.0643\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.9855 - acc: 0.2663 - val_loss: 4.5142 - val_acc: 0.0693\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.3610 - acc: 0.4002 - val_loss: 4.5912 - val_acc: 0.0780\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8209 - acc: 0.0100 - val_loss: 4.7865 - val_acc: 0.0127\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6889 - acc: 0.0257 - val_loss: 4.7413 - val_acc: 0.0203\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.3003 - acc: 0.0683 - val_loss: 4.7037 - val_acc: 0.0273\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.6847 - acc: 0.1480 - val_loss: 4.4711 - val_acc: 0.0517\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.0108 - acc: 0.2677 - val_loss: 4.5284 - val_acc: 0.0700\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.2507 - acc: 0.4183 - val_loss: 4.8054 - val_acc: 0.0890\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8162 - acc: 0.0082 - val_loss: 4.7750 - val_acc: 0.0140\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6487 - acc: 0.0267 - val_loss: 4.7064 - val_acc: 0.0210\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.2977 - acc: 0.0650 - val_loss: 4.6071 - val_acc: 0.0340\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.6732 - acc: 0.1457 - val_loss: 4.6233 - val_acc: 0.0367\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.9763 - acc: 0.2723 - val_loss: 4.3048 - val_acc: 0.0707\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.2859 - acc: 0.4238 - val_loss: 4.7397 - val_acc: 0.0853\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.6187 - acc: 0.5722 - val_loss: 4.5049 - val_acc: 0.1030\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8209 - acc: 0.0087 - val_loss: 4.7760 - val_acc: 0.0133\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6358 - acc: 0.0312 - val_loss: 4.6834 - val_acc: 0.0273\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.2384 - acc: 0.0688 - val_loss: 4.6197 - val_acc: 0.0297\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 3.6745 - acc: 0.1458 - val_loss: 4.4601 - val_acc: 0.0467\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.0719 - acc: 0.2577 - val_loss: 4.3301 - val_acc: 0.0750\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.3779 - acc: 0.3935 - val_loss: 4.4877 - val_acc: 0.0903\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.7710 - acc: 0.5223 - val_loss: 4.7553 - val_acc: 0.0960\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8004 - acc: 0.0155 - val_loss: 4.7831 - val_acc: 0.0117\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.5273 - acc: 0.0465 - val_loss: 4.7374 - val_acc: 0.0263\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.8150 - acc: 0.1303 - val_loss: 4.9226 - val_acc: 0.0307\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.0584 - acc: 0.2585 - val_loss: 4.7970 - val_acc: 0.0503\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8091 - acc: 0.0092 - val_loss: 4.7779 - val_acc: 0.0147\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.5621 - acc: 0.0467 - val_loss: 4.7956 - val_acc: 0.0163\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.8371 - acc: 0.1185 - val_loss: 4.7315 - val_acc: 0.0400\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.0297 - acc: 0.2493 - val_loss: 4.5382 - val_acc: 0.0537\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.2893 - acc: 0.4043 - val_loss: 4.6837 - val_acc: 0.0613\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.6743 - acc: 0.5438 - val_loss: 5.0366 - val_acc: 0.0650\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8116 - acc: 0.0118 - val_loss: 4.7799 - val_acc: 0.0123\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.5733 - acc: 0.0437 - val_loss: 4.8017 - val_acc: 0.0177\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.9423 - acc: 0.0993 - val_loss: 4.8498 - val_acc: 0.0373\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 4.8046 - acc: 0.0120 - val_loss: 4.7866 - val_acc: 0.0150\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 4.5892 - acc: 0.0427 - val_loss: 4.7084 - val_acc: 0.0190\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 3.9599 - acc: 0.1057 - val_loss: 4.7424 - val_acc: 0.0353\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.2515 - acc: 0.2207 - val_loss: 4.5727 - val_acc: 0.0493\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 2.5603 - acc: 0.3468 - val_loss: 4.5761 - val_acc: 0.0643\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 1.8718 - acc: 0.4957 - val_loss: 5.0596 - val_acc: 0.0617\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8164 - acc: 0.0110 - val_loss: 4.7854 - val_acc: 0.0117\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6440 - acc: 0.0380 - val_loss: 4.7139 - val_acc: 0.0260\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.0181 - acc: 0.0997 - val_loss: 4.7512 - val_acc: 0.0350\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.1950 - acc: 0.2190 - val_loss: 4.7525 - val_acc: 0.0510\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.7995 - acc: 0.0110 - val_loss: 4.7863 - val_acc: 0.0107\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.5420 - acc: 0.0453 - val_loss: 4.7971 - val_acc: 0.0203\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.8372 - acc: 0.1265 - val_loss: 4.7358 - val_acc: 0.0343\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.9352 - acc: 0.2743 - val_loss: 4.7540 - val_acc: 0.0453\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.1689 - acc: 0.4358 - val_loss: 5.0011 - val_acc: 0.0580\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8091 - acc: 0.0093 - val_loss: 4.7776 - val_acc: 0.0103\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.5871 - acc: 0.0388 - val_loss: 4.7034 - val_acc: 0.0217\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.9688 - acc: 0.1023 - val_loss: 4.7690 - val_acc: 0.0293\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.2888 - acc: 0.2112 - val_loss: 4.7564 - val_acc: 0.0440\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8031 - acc: 0.0128 - val_loss: 4.7766 - val_acc: 0.0133\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.4911 - acc: 0.0468 - val_loss: 4.7374 - val_acc: 0.0213\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.7761 - acc: 0.1342 - val_loss: 4.7236 - val_acc: 0.0370\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.8969 - acc: 0.2780 - val_loss: 4.5663 - val_acc: 0.0553\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.1536 - acc: 0.4343 - val_loss: 4.6535 - val_acc: 0.0723\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.5252 - acc: 0.5880 - val_loss: 5.0637 - val_acc: 0.0790\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8232 - acc: 0.0087 - val_loss: 4.7738 - val_acc: 0.0113\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6747 - acc: 0.0320 - val_loss: 4.6702 - val_acc: 0.0247\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.2159 - acc: 0.0872 - val_loss: 4.3577 - val_acc: 0.0570\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.3487 - acc: 0.2112 - val_loss: 3.9586 - val_acc: 0.1050\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.4964 - acc: 0.3768 - val_loss: 3.8513 - val_acc: 0.1513\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.7009 - acc: 0.5457 - val_loss: 4.3074 - val_acc: 0.1517\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.0148 - acc: 0.7173 - val_loss: 4.2254 - val_acc: 0.1573\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8319 - acc: 0.0078 - val_loss: 4.7797 - val_acc: 0.0130\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.7310 - acc: 0.0188 - val_loss: 4.7296 - val_acc: 0.0193\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.4912 - acc: 0.0488 - val_loss: 4.6067 - val_acc: 0.0287\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.9426 - acc: 0.1130 - val_loss: 4.2575 - val_acc: 0.0740\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.1941 - acc: 0.2315 - val_loss: 4.2068 - val_acc: 0.0863\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.4733 - acc: 0.3768 - val_loss: 4.0289 - val_acc: 0.1220\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.7577 - acc: 0.5468 - val_loss: 4.1178 - val_acc: 0.1240\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.2010 - acc: 0.6762 - val_loss: 5.0176 - val_acc: 0.1333\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8209 - acc: 0.0120 - val_loss: 4.7738 - val_acc: 0.0147\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6627 - acc: 0.0332 - val_loss: 4.6830 - val_acc: 0.0227\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.1843 - acc: 0.0838 - val_loss: 4.3754 - val_acc: 0.0667\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.3970 - acc: 0.2012 - val_loss: 4.0271 - val_acc: 0.1000\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.5298 - acc: 0.3602 - val_loss: 3.9278 - val_acc: 0.1310\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.7411 - acc: 0.5397 - val_loss: 4.3220 - val_acc: 0.1490\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.0572 - acc: 0.7103 - val_loss: 4.5549 - val_acc: 0.1523\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8177 - acc: 0.0117 - val_loss: 4.7732 - val_acc: 0.0123\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6592 - acc: 0.0307 - val_loss: 4.6600 - val_acc: 0.0207\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.2033 - acc: 0.0822 - val_loss: 4.3304 - val_acc: 0.0660\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.3723 - acc: 0.2083 - val_loss: 3.9649 - val_acc: 0.1190\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.5138 - acc: 0.3592 - val_loss: 3.9809 - val_acc: 0.1377\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.7608 - acc: 0.5382 - val_loss: 3.9700 - val_acc: 0.1560\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8204 - acc: 0.0082 - val_loss: 4.7694 - val_acc: 0.0163\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6768 - acc: 0.0322 - val_loss: 4.6709 - val_acc: 0.0247\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.2599 - acc: 0.0798 - val_loss: 4.4138 - val_acc: 0.0507\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.5434 - acc: 0.1725 - val_loss: 4.0455 - val_acc: 0.1050\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.7294 - acc: 0.3283 - val_loss: 3.8750 - val_acc: 0.1387\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.0089 - acc: 0.4787 - val_loss: 3.9825 - val_acc: 0.1443\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.2254 - acc: 0.6652 - val_loss: 4.0134 - val_acc: 0.1757\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8142 - acc: 0.0085 - val_loss: 4.7691 - val_acc: 0.0123\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 4.6294 - acc: 0.0335 - val_loss: 4.6366 - val_acc: 0.0287\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 4.1162 - acc: 0.0918 - val_loss: 4.4452 - val_acc: 0.0457\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 3.3418 - acc: 0.2130 - val_loss: 4.0848 - val_acc: 0.1000\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 2.5070 - acc: 0.3698 - val_loss: 4.0758 - val_acc: 0.1197\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.8058 - acc: 0.5217 - val_loss: 3.9448 - val_acc: 0.1497\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 28s 5ms/step - loss: 1.0592 - acc: 0.7075 - val_loss: 4.2670 - val_acc: 0.1637\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.6163 - acc: 0.8292 - val_loss: 4.9365 - val_acc: 0.1590\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8127 - acc: 0.0092 - val_loss: 4.7682 - val_acc: 0.0170\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6154 - acc: 0.0352 - val_loss: 4.6316 - val_acc: 0.0300\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.0593 - acc: 0.0972 - val_loss: 4.3716 - val_acc: 0.0617\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.2784 - acc: 0.2263 - val_loss: 3.9656 - val_acc: 0.1097\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.4515 - acc: 0.3723 - val_loss: 3.8951 - val_acc: 0.1410\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.6297 - acc: 0.5703 - val_loss: 3.8370 - val_acc: 0.1613\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.9848 - acc: 0.7242 - val_loss: 4.9106 - val_acc: 0.1413\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.5661 - acc: 0.8397 - val_loss: 5.0802 - val_acc: 0.1647\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8141 - acc: 0.0102 - val_loss: 4.7712 - val_acc: 0.0133\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6350 - acc: 0.0313 - val_loss: 4.6388 - val_acc: 0.0320\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.0804 - acc: 0.1077 - val_loss: 4.3358 - val_acc: 0.0610\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.3341 - acc: 0.2098 - val_loss: 4.0284 - val_acc: 0.0977\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.4772 - acc: 0.3683 - val_loss: 3.8316 - val_acc: 0.1317\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.7062 - acc: 0.5448 - val_loss: 4.0509 - val_acc: 0.1373\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.0348 - acc: 0.7140 - val_loss: 4.8889 - val_acc: 0.1470\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8065 - acc: 0.0127 - val_loss: 4.7651 - val_acc: 0.0150\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 4.5326 - acc: 0.0498 - val_loss: 4.6316 - val_acc: 0.0400\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 3.7085 - acc: 0.1392 - val_loss: 4.4431 - val_acc: 0.0620\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 2.6361 - acc: 0.3302 - val_loss: 4.3559 - val_acc: 0.1087\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 1.8629 - acc: 0.4987 - val_loss: 4.0048 - val_acc: 0.1283\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 1.1479 - acc: 0.6833 - val_loss: 4.3846 - val_acc: 0.1373\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 0.6128 - acc: 0.8275 - val_loss: 5.0132 - val_acc: 0.1483\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8265 - acc: 0.0092 - val_loss: 4.7806 - val_acc: 0.0147\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 4.6633 - acc: 0.0352 - val_loss: 4.6742 - val_acc: 0.0287\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 4.0236 - acc: 0.1137 - val_loss: 4.3261 - val_acc: 0.0703\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.9275 - acc: 0.2725 - val_loss: 4.0276 - val_acc: 0.1113\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 2.0093 - acc: 0.4555 - val_loss: 4.1562 - val_acc: 0.1440\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.2922 - acc: 0.6425 - val_loss: 4.1540 - val_acc: 0.1503\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8018 - acc: 0.0133 - val_loss: 4.7633 - val_acc: 0.0140\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 4.5387 - acc: 0.0458 - val_loss: 4.6796 - val_acc: 0.0283\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.7255 - acc: 0.1387 - val_loss: 4.4059 - val_acc: 0.0670\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.8388 - acc: 0.2973 - val_loss: 4.2853 - val_acc: 0.0920\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 1.9955 - acc: 0.4737 - val_loss: 4.2603 - val_acc: 0.1217\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.2594 - acc: 0.6520 - val_loss: 4.8277 - val_acc: 0.1277\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 0.8482 - acc: 0.7553 - val_loss: 4.9646 - val_acc: 0.1203\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8105 - acc: 0.0095 - val_loss: 4.7714 - val_acc: 0.0090\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.5904 - acc: 0.0470 - val_loss: 4.6498 - val_acc: 0.0293\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.8534 - acc: 0.1168 - val_loss: 4.3623 - val_acc: 0.0593\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.9274 - acc: 0.2873 - val_loss: 4.2131 - val_acc: 0.0913\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.0949 - acc: 0.4495 - val_loss: 4.0443 - val_acc: 0.1253\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.3959 - acc: 0.6158 - val_loss: 4.6025 - val_acc: 0.1223\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.8698 - acc: 0.7538 - val_loss: 4.6801 - val_acc: 0.1087\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8180 - acc: 0.0098 - val_loss: 4.7731 - val_acc: 0.0127\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.5870 - acc: 0.0418 - val_loss: 4.6981 - val_acc: 0.0207\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.8628 - acc: 0.1222 - val_loss: 4.3841 - val_acc: 0.0623\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.0696 - acc: 0.2438 - val_loss: 4.2885 - val_acc: 0.0813\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.2150 - acc: 0.4193 - val_loss: 4.3765 - val_acc: 0.0953\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.4852 - acc: 0.5875 - val_loss: 4.5046 - val_acc: 0.1073\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8117 - acc: 0.0093 - val_loss: 4.7745 - val_acc: 0.0140\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.5899 - acc: 0.0417 - val_loss: 4.6615 - val_acc: 0.0313\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.8668 - acc: 0.1210 - val_loss: 4.4796 - val_acc: 0.0603\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.9821 - acc: 0.2670 - val_loss: 4.1887 - val_acc: 0.0930\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.2180 - acc: 0.4162 - val_loss: 4.3046 - val_acc: 0.1027\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.4781 - acc: 0.5917 - val_loss: 4.6586 - val_acc: 0.1203\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8151 - acc: 0.0088 - val_loss: 4.7718 - val_acc: 0.0107\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.5773 - acc: 0.0428 - val_loss: 4.6162 - val_acc: 0.0297\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.8218 - acc: 0.1327 - val_loss: 4.4099 - val_acc: 0.0637\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.8230 - acc: 0.3007 - val_loss: 4.1226 - val_acc: 0.0963\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.9921 - acc: 0.4720 - val_loss: 4.1025 - val_acc: 0.1260\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.3599 - acc: 0.6248 - val_loss: 4.1372 - val_acc: 0.1410\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.7376 - acc: 0.7960 - val_loss: 4.8083 - val_acc: 0.1473\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8049 - acc: 0.0120 - val_loss: 4.7609 - val_acc: 0.0150\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.4930 - acc: 0.0517 - val_loss: 4.6404 - val_acc: 0.0323\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.6591 - acc: 0.1530 - val_loss: 4.3913 - val_acc: 0.0763\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.6805 - acc: 0.3195 - val_loss: 4.1204 - val_acc: 0.1107\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.9003 - acc: 0.4985 - val_loss: 4.1687 - val_acc: 0.1310\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.1599 - acc: 0.6752 - val_loss: 4.8682 - val_acc: 0.1107\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8286 - acc: 0.0095 - val_loss: 4.7771 - val_acc: 0.0137\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.7078 - acc: 0.0267 - val_loss: 4.6985 - val_acc: 0.0353\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.3947 - acc: 0.0785 - val_loss: 4.4534 - val_acc: 0.0590\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.6647 - acc: 0.1625 - val_loss: 3.9587 - val_acc: 0.1050\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.7986 - acc: 0.3110 - val_loss: 3.8377 - val_acc: 0.1347\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.0428 - acc: 0.4690 - val_loss: 3.6137 - val_acc: 0.1793\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.2524 - acc: 0.6587 - val_loss: 3.9483 - val_acc: 0.1950\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.7783 - acc: 0.7848 - val_loss: 4.4373 - val_acc: 0.1963\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8283 - acc: 0.0077 - val_loss: 4.7756 - val_acc: 0.0133\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.7132 - acc: 0.0220 - val_loss: 4.6992 - val_acc: 0.0280\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.4168 - acc: 0.0668 - val_loss: 4.4228 - val_acc: 0.0637\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.6789 - acc: 0.1635 - val_loss: 4.0511 - val_acc: 0.1033\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.7459 - acc: 0.3183 - val_loss: 3.7437 - val_acc: 0.1483\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.9913 - acc: 0.4858 - val_loss: 3.7110 - val_acc: 0.1713\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.3424 - acc: 0.6420 - val_loss: 3.9390 - val_acc: 0.1767\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.7455 - acc: 0.7872 - val_loss: 4.0582 - val_acc: 0.1937\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8182 - acc: 0.0117 - val_loss: 4.7598 - val_acc: 0.0177\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6744 - acc: 0.0308 - val_loss: 4.6299 - val_acc: 0.0290\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.2259 - acc: 0.0907 - val_loss: 4.2160 - val_acc: 0.0820\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.2826 - acc: 0.2272 - val_loss: 3.7195 - val_acc: 0.1453\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.3555 - acc: 0.3987 - val_loss: 3.4872 - val_acc: 0.1780\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.5796 - acc: 0.5773 - val_loss: 3.5118 - val_acc: 0.1973\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.0227 - acc: 0.7238 - val_loss: 3.9512 - val_acc: 0.1977\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8158 - acc: 0.0095 - val_loss: 4.7454 - val_acc: 0.0153\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6102 - acc: 0.0382 - val_loss: 4.5159 - val_acc: 0.0537\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.8940 - acc: 0.1268 - val_loss: 3.9424 - val_acc: 0.1033\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.8448 - acc: 0.2945 - val_loss: 3.4633 - val_acc: 0.1843\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.0103 - acc: 0.4728 - val_loss: 3.4821 - val_acc: 0.2003\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.2769 - acc: 0.6533 - val_loss: 3.5797 - val_acc: 0.2190\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8247 - acc: 0.0058 - val_loss: 4.7766 - val_acc: 0.0150\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.7248 - acc: 0.0227 - val_loss: 4.7103 - val_acc: 0.0257\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.4598 - acc: 0.0553 - val_loss: 4.4376 - val_acc: 0.0480\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.8218 - acc: 0.1340 - val_loss: 4.1327 - val_acc: 0.0917\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.0134 - acc: 0.2750 - val_loss: 4.0032 - val_acc: 0.1173\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.3555 - acc: 0.4023 - val_loss: 3.7309 - val_acc: 0.1547\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.6718 - acc: 0.5515 - val_loss: 3.7542 - val_acc: 0.1740\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.9716 - acc: 0.7368 - val_loss: 4.2374 - val_acc: 0.1890\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8210 - acc: 0.0077 - val_loss: 4.7723 - val_acc: 0.0150\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6999 - acc: 0.0283 - val_loss: 4.6817 - val_acc: 0.0297\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.3440 - acc: 0.0725 - val_loss: 4.4046 - val_acc: 0.0540\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.6379 - acc: 0.1683 - val_loss: 4.0030 - val_acc: 0.1003\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.8190 - acc: 0.3108 - val_loss: 3.8291 - val_acc: 0.1380\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 1.9830 - acc: 0.4865 - val_loss: 3.7863 - val_acc: 0.1577\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.3192 - acc: 0.6430 - val_loss: 3.9611 - val_acc: 0.1730\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.7761 - acc: 0.7792 - val_loss: 4.1369 - val_acc: 0.1830\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8311 - acc: 0.0090 - val_loss: 4.7809 - val_acc: 0.0133\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.7399 - acc: 0.0213 - val_loss: 4.7212 - val_acc: 0.0223\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.5416 - acc: 0.0492 - val_loss: 4.4879 - val_acc: 0.0537\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.9606 - acc: 0.1145 - val_loss: 4.1759 - val_acc: 0.0743\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.1545 - acc: 0.2523 - val_loss: 3.8268 - val_acc: 0.1177\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.4519 - acc: 0.3798 - val_loss: 3.5858 - val_acc: 0.1643\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.6411 - acc: 0.5672 - val_loss: 3.8982 - val_acc: 0.1737\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.0820 - acc: 0.7063 - val_loss: 4.4628 - val_acc: 0.1733\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8276 - acc: 0.0090 - val_loss: 4.7800 - val_acc: 0.0127\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.7232 - acc: 0.0235 - val_loss: 4.7058 - val_acc: 0.0267\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.4733 - acc: 0.0613 - val_loss: 4.5163 - val_acc: 0.0393\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.8212 - acc: 0.1378 - val_loss: 4.1719 - val_acc: 0.0890\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.9263 - acc: 0.2895 - val_loss: 3.8923 - val_acc: 0.1323\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.0837 - acc: 0.4637 - val_loss: 3.8346 - val_acc: 0.1587\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.4397 - acc: 0.6125 - val_loss: 3.9173 - val_acc: 0.1847\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.7348 - acc: 0.7943 - val_loss: 4.2461 - val_acc: 0.1677\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8135 - acc: 0.0098 - val_loss: 4.7684 - val_acc: 0.0137\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6149 - acc: 0.0382 - val_loss: 4.5720 - val_acc: 0.0410\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.9416 - acc: 0.1083 - val_loss: 4.1756 - val_acc: 0.0797\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.9570 - acc: 0.2740 - val_loss: 3.7418 - val_acc: 0.1483\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.0526 - acc: 0.4510 - val_loss: 4.2369 - val_acc: 0.1513\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.3431 - acc: 0.6288 - val_loss: 3.8932 - val_acc: 0.1883\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8271 - acc: 0.0093 - val_loss: 4.7742 - val_acc: 0.0110\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6742 - acc: 0.0305 - val_loss: 4.6652 - val_acc: 0.0303\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.1105 - acc: 0.0945 - val_loss: 4.3941 - val_acc: 0.0717\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.1282 - acc: 0.2337 - val_loss: 3.8673 - val_acc: 0.1380\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.2330 - acc: 0.4167 - val_loss: 3.8405 - val_acc: 0.1667\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.4973 - acc: 0.5845 - val_loss: 3.9504 - val_acc: 0.1733\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.8661 - acc: 0.7507 - val_loss: 3.8441 - val_acc: 0.2083\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.7993 - acc: 0.0130 - val_loss: 4.7272 - val_acc: 0.0193\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.4430 - acc: 0.0600 - val_loss: 4.3175 - val_acc: 0.0670\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.3385 - acc: 0.2038 - val_loss: 3.8294 - val_acc: 0.1320\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.2002 - acc: 0.4188 - val_loss: 3.5080 - val_acc: 0.1783\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.4508 - acc: 0.6028 - val_loss: 3.6100 - val_acc: 0.2020\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.8998 - acc: 0.7450 - val_loss: 3.7854 - val_acc: 0.2387\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8236 - acc: 0.0093 - val_loss: 4.7783 - val_acc: 0.0137\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.7021 - acc: 0.0305 - val_loss: 4.6915 - val_acc: 0.0280\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.2049 - acc: 0.0822 - val_loss: 4.4161 - val_acc: 0.0557\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.2724 - acc: 0.2202 - val_loss: 3.9848 - val_acc: 0.1233\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.2388 - acc: 0.4117 - val_loss: 3.9766 - val_acc: 0.1587\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.4706 - acc: 0.5893 - val_loss: 3.8581 - val_acc: 0.1840\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.9062 - acc: 0.7432 - val_loss: 4.3569 - val_acc: 0.1823\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.4803 - acc: 0.8585 - val_loss: 4.6894 - val_acc: 0.1880\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.8119 - acc: 0.0097 - val_loss: 4.7628 - val_acc: 0.0197\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.6024 - acc: 0.0442 - val_loss: 4.5853 - val_acc: 0.0360\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 3.8184 - acc: 0.1320 - val_loss: 4.1867 - val_acc: 0.0880\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 2.7639 - acc: 0.3082 - val_loss: 3.7822 - val_acc: 0.1560\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 1.8769 - acc: 0.4998 - val_loss: 3.8355 - val_acc: 0.1783\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 1.1571 - acc: 0.6750 - val_loss: 4.0320 - val_acc: 0.1920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8187 - acc: 0.0100 - val_loss: 4.7641 - val_acc: 0.0077\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6115 - acc: 0.0387 - val_loss: 4.5671 - val_acc: 0.0387\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.8851 - acc: 0.1238 - val_loss: 4.1693 - val_acc: 0.1070\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.6292 - acc: 0.3275 - val_loss: 3.7764 - val_acc: 0.1537\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.7489 - acc: 0.5258 - val_loss: 3.6446 - val_acc: 0.1847\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.0587 - acc: 0.6957 - val_loss: 3.9579 - val_acc: 0.2087\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.5326 - acc: 0.8465 - val_loss: 4.3480 - val_acc: 0.2193\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8130 - acc: 0.0103 - val_loss: 4.7492 - val_acc: 0.0187\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.5282 - acc: 0.0543 - val_loss: 4.4802 - val_acc: 0.0517\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.5246 - acc: 0.1708 - val_loss: 3.8138 - val_acc: 0.1217\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.4082 - acc: 0.3753 - val_loss: 3.6417 - val_acc: 0.1703\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.5983 - acc: 0.5628 - val_loss: 3.6362 - val_acc: 0.1940\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.0082 - acc: 0.7135 - val_loss: 3.6705 - val_acc: 0.2170\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.5188 - acc: 0.8462 - val_loss: 4.0787 - val_acc: 0.1960\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8148 - acc: 0.0095 - val_loss: 4.7782 - val_acc: 0.0123\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6385 - acc: 0.0345 - val_loss: 4.6199 - val_acc: 0.0307\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.9974 - acc: 0.1118 - val_loss: 4.1113 - val_acc: 0.0980\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.8461 - acc: 0.2895 - val_loss: 3.7582 - val_acc: 0.1453\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.9354 - acc: 0.4825 - val_loss: 3.5842 - val_acc: 0.1947\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.2783 - acc: 0.6443 - val_loss: 3.9646 - val_acc: 0.1900\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.6737 - acc: 0.8050 - val_loss: 4.0506 - val_acc: 0.1923\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.8218 - acc: 0.0097 - val_loss: 4.7713 - val_acc: 0.0143\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.7105 - acc: 0.0307 - val_loss: 4.6924 - val_acc: 0.0350\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.4256 - acc: 0.0742 - val_loss: 4.3984 - val_acc: 0.0650\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.6996 - acc: 0.1727 - val_loss: 3.9286 - val_acc: 0.1210\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.8318 - acc: 0.3100 - val_loss: 3.7062 - val_acc: 0.1573\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.0799 - acc: 0.4740 - val_loss: 3.6054 - val_acc: 0.2020\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.3584 - acc: 0.6387 - val_loss: 3.8519 - val_acc: 0.2043\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.7491 - acc: 0.7975 - val_loss: 4.2190 - val_acc: 0.1973\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.8187 - acc: 0.0095 - val_loss: 4.7649 - val_acc: 0.0133\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6715 - acc: 0.0407 - val_loss: 4.6260 - val_acc: 0.0390\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.2395 - acc: 0.0925 - val_loss: 4.1819 - val_acc: 0.0883\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 3.3689 - acc: 0.2195 - val_loss: 3.6558 - val_acc: 0.1537\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.4973 - acc: 0.3763 - val_loss: 3.5437 - val_acc: 0.1813\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 1.8056 - acc: 0.5275 - val_loss: 3.5713 - val_acc: 0.2080\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 1.0662 - acc: 0.7008 - val_loss: 4.0387 - val_acc: 0.2183\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.8244 - acc: 0.0080 - val_loss: 4.7695 - val_acc: 0.0157\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6693 - acc: 0.0340 - val_loss: 4.6235 - val_acc: 0.0357\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.1653 - acc: 0.0970 - val_loss: 4.1252 - val_acc: 0.0963\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.2372 - acc: 0.2403 - val_loss: 3.6193 - val_acc: 0.1523\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.4369 - acc: 0.3863 - val_loss: 3.7228 - val_acc: 0.1713\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.6834 - acc: 0.5535 - val_loss: 3.5335 - val_acc: 0.2063\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.9978 - acc: 0.7297 - val_loss: 4.0036 - val_acc: 0.2053\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.5231 - acc: 0.8563 - val_loss: 4.1614 - val_acc: 0.2217\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8232 - acc: 0.0097 - val_loss: 4.7704 - val_acc: 0.0107\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6954 - acc: 0.0318 - val_loss: 4.6466 - val_acc: 0.0407\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.3070 - acc: 0.0858 - val_loss: 4.2691 - val_acc: 0.0790\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.5142 - acc: 0.1875 - val_loss: 3.8325 - val_acc: 0.1427\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.7125 - acc: 0.3338 - val_loss: 3.6135 - val_acc: 0.1600\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.9073 - acc: 0.5105 - val_loss: 3.5994 - val_acc: 0.2020\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.2624 - acc: 0.6607 - val_loss: 3.5621 - val_acc: 0.2060\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.6993 - acc: 0.8090 - val_loss: 4.2248 - val_acc: 0.2030\n",
      "Epoch 9/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.3462 - acc: 0.9048 - val_loss: 4.3883 - val_acc: 0.2237\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.8267 - acc: 0.0092 - val_loss: 4.7737 - val_acc: 0.0123\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.7184 - acc: 0.0253 - val_loss: 4.6891 - val_acc: 0.0273\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.4332 - acc: 0.0680 - val_loss: 4.3616 - val_acc: 0.0710\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 3.6459 - acc: 0.1697 - val_loss: 3.8454 - val_acc: 0.1260\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.7804 - acc: 0.3140 - val_loss: 3.7836 - val_acc: 0.1533\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.9510 - acc: 0.5017 - val_loss: 3.6993 - val_acc: 0.1800\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.3015 - acc: 0.6528 - val_loss: 4.0702 - val_acc: 0.1910\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.8361 - acc: 0.7677 - val_loss: 4.4138 - val_acc: 0.2023\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.8284 - acc: 0.0078 - val_loss: 4.7620 - val_acc: 0.0177\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.6959 - acc: 0.0315 - val_loss: 4.6566 - val_acc: 0.0353\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.3387 - acc: 0.0743 - val_loss: 4.2982 - val_acc: 0.0723\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.5120 - acc: 0.1895 - val_loss: 3.8512 - val_acc: 0.1303\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 2.6382 - acc: 0.3463 - val_loss: 3.5517 - val_acc: 0.1763\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.9082 - acc: 0.5067 - val_loss: 3.8794 - val_acc: 0.1820\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.1909 - acc: 0.6798 - val_loss: 3.8586 - val_acc: 0.2120\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8294 - acc: 0.0088 - val_loss: 4.7801 - val_acc: 0.0143\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.7109 - acc: 0.0270 - val_loss: 4.6835 - val_acc: 0.0317\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.3902 - acc: 0.0725 - val_loss: 4.3428 - val_acc: 0.0640\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.6213 - acc: 0.1663 - val_loss: 3.8269 - val_acc: 0.1370\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.7845 - acc: 0.3183 - val_loss: 3.6360 - val_acc: 0.1570\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.0314 - acc: 0.4695 - val_loss: 3.4147 - val_acc: 0.2123\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.2977 - acc: 0.6452 - val_loss: 3.9362 - val_acc: 0.1893\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.8974 - acc: 0.7580 - val_loss: 4.0317 - val_acc: 0.2073\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8262 - acc: 0.0072 - val_loss: 4.7679 - val_acc: 0.0133\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6963 - acc: 0.0262 - val_loss: 4.6471 - val_acc: 0.0333\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.3040 - acc: 0.0755 - val_loss: 4.2136 - val_acc: 0.0777\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.4775 - acc: 0.1982 - val_loss: 3.7015 - val_acc: 0.1493\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.5841 - acc: 0.3593 - val_loss: 3.6666 - val_acc: 0.1660\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.8987 - acc: 0.5037 - val_loss: 3.4424 - val_acc: 0.2047\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.2262 - acc: 0.6695 - val_loss: 3.7390 - val_acc: 0.2200\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.6715 - acc: 0.8120 - val_loss: 4.2425 - val_acc: 0.2123\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.8167 - acc: 0.0105 - val_loss: 4.7439 - val_acc: 0.0177\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.5785 - acc: 0.0433 - val_loss: 4.4480 - val_acc: 0.0530\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.6914 - acc: 0.1438 - val_loss: 3.8099 - val_acc: 0.1330\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.5620 - acc: 0.3435 - val_loss: 3.4121 - val_acc: 0.1980\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.8257 - acc: 0.5125 - val_loss: 3.2351 - val_acc: 0.2420\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.1499 - acc: 0.6762 - val_loss: 3.3552 - val_acc: 0.2537\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.6230 - acc: 0.8182 - val_loss: 3.8278 - val_acc: 0.2577\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8225 - acc: 0.0097 - val_loss: 4.7685 - val_acc: 0.0147\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6621 - acc: 0.0355 - val_loss: 4.6172 - val_acc: 0.0283\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.0434 - acc: 0.1058 - val_loss: 4.0718 - val_acc: 0.1007\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.8967 - acc: 0.2753 - val_loss: 3.6277 - val_acc: 0.1633\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.0201 - acc: 0.4597 - val_loss: 3.5554 - val_acc: 0.2147\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.2742 - acc: 0.6422 - val_loss: 3.3125 - val_acc: 0.2307\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.7976 - acc: 0.7720 - val_loss: 3.6633 - val_acc: 0.2403\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.3817 - acc: 0.8885 - val_loss: 4.1699 - val_acc: 0.2323\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8064 - acc: 0.0133 - val_loss: 4.7416 - val_acc: 0.0150\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.5454 - acc: 0.0480 - val_loss: 4.4457 - val_acc: 0.0497\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.7140 - acc: 0.1405 - val_loss: 4.0042 - val_acc: 0.1047\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.6710 - acc: 0.3207 - val_loss: 3.5651 - val_acc: 0.1770\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.9226 - acc: 0.4830 - val_loss: 3.4443 - val_acc: 0.2180\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.3105 - acc: 0.6342 - val_loss: 3.7162 - val_acc: 0.2043\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.7783 - acc: 0.7790 - val_loss: 4.1538 - val_acc: 0.2197\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.8038 - acc: 0.0127 - val_loss: 4.7394 - val_acc: 0.0190\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.5299 - acc: 0.0555 - val_loss: 4.3619 - val_acc: 0.0620\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 3.4625 - acc: 0.1972 - val_loss: 3.6447 - val_acc: 0.1567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 2.3230 - acc: 0.3900 - val_loss: 3.3252 - val_acc: 0.2173\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 1.5325 - acc: 0.5790 - val_loss: 3.4949 - val_acc: 0.2277\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 0.9508 - acc: 0.7230 - val_loss: 3.6569 - val_acc: 0.2447\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8182 - acc: 0.0078 - val_loss: 4.7697 - val_acc: 0.0173\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6977 - acc: 0.0310 - val_loss: 4.6778 - val_acc: 0.0313\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.2433 - acc: 0.0863 - val_loss: 4.2340 - val_acc: 0.0870\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.2543 - acc: 0.2313 - val_loss: 3.8414 - val_acc: 0.1237\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.3500 - acc: 0.3882 - val_loss: 3.5882 - val_acc: 0.1737\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.6557 - acc: 0.5468 - val_loss: 3.6955 - val_acc: 0.1933\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.9968 - acc: 0.7142 - val_loss: 3.8979 - val_acc: 0.2247\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8080 - acc: 0.0120 - val_loss: 4.7546 - val_acc: 0.0150\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6049 - acc: 0.0433 - val_loss: 4.5363 - val_acc: 0.0417\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.9034 - acc: 0.1227 - val_loss: 3.9637 - val_acc: 0.1067\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.8052 - acc: 0.3015 - val_loss: 3.5999 - val_acc: 0.1750\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.0038 - acc: 0.4682 - val_loss: 3.3987 - val_acc: 0.2097\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.3792 - acc: 0.6247 - val_loss: 3.5351 - val_acc: 0.2263\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.8635 - acc: 0.7575 - val_loss: 3.9424 - val_acc: 0.2157\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8177 - acc: 0.0087 - val_loss: 4.7550 - val_acc: 0.0173\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6071 - acc: 0.0467 - val_loss: 4.5106 - val_acc: 0.0563\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.7259 - acc: 0.1498 - val_loss: 3.7672 - val_acc: 0.1290\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.5295 - acc: 0.3475 - val_loss: 3.3956 - val_acc: 0.2073\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.6955 - acc: 0.5337 - val_loss: 3.2919 - val_acc: 0.2330\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.0310 - acc: 0.7060 - val_loss: 3.5322 - val_acc: 0.2560\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.5414 - acc: 0.8410 - val_loss: 3.5709 - val_acc: 0.2633\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8057 - acc: 0.0110 - val_loss: 4.7405 - val_acc: 0.0177\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.4825 - acc: 0.0630 - val_loss: 4.2809 - val_acc: 0.0677\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.3278 - acc: 0.2028 - val_loss: 3.5674 - val_acc: 0.1723\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.1414 - acc: 0.4307 - val_loss: 3.3471 - val_acc: 0.2260\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.3369 - acc: 0.6225 - val_loss: 3.3141 - val_acc: 0.2467\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.8334 - acc: 0.7662 - val_loss: 3.5642 - val_acc: 0.2507\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.4094 - acc: 0.8782 - val_loss: 4.1590 - val_acc: 0.2563\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.8292 - acc: 0.0087 - val_loss: 4.7810 - val_acc: 0.0117\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.7322 - acc: 0.0195 - val_loss: 4.7270 - val_acc: 0.0253\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.5301 - acc: 0.0555 - val_loss: 4.5007 - val_acc: 0.0553\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 3.9249 - acc: 0.1292 - val_loss: 4.1033 - val_acc: 0.0970\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 3.0457 - acc: 0.2770 - val_loss: 3.7780 - val_acc: 0.1447\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.3529 - acc: 0.4113 - val_loss: 3.8488 - val_acc: 0.1687\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.6351 - acc: 0.5730 - val_loss: 4.1230 - val_acc: 0.1633\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.8255 - acc: 0.0082 - val_loss: 4.7709 - val_acc: 0.0100\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.7186 - acc: 0.0218 - val_loss: 4.7018 - val_acc: 0.0287\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.4975 - acc: 0.0602 - val_loss: 4.4866 - val_acc: 0.0550\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 3.8870 - acc: 0.1355 - val_loss: 4.0630 - val_acc: 0.1037\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 3.1020 - acc: 0.2610 - val_loss: 3.8572 - val_acc: 0.1397\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.3158 - acc: 0.4303 - val_loss: 3.8095 - val_acc: 0.1673\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.6036 - acc: 0.5823 - val_loss: 4.0849 - val_acc: 0.1763\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 0.9457 - acc: 0.7405 - val_loss: 4.1632 - val_acc: 0.1747\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8170 - acc: 0.0072 - val_loss: 4.7639 - val_acc: 0.0100\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6800 - acc: 0.0320 - val_loss: 4.6392 - val_acc: 0.0323\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.3291 - acc: 0.0755 - val_loss: 4.2619 - val_acc: 0.0777\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.6113 - acc: 0.1760 - val_loss: 3.8899 - val_acc: 0.1223\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.8081 - acc: 0.3187 - val_loss: 3.7259 - val_acc: 0.1620\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.1018 - acc: 0.4685 - val_loss: 3.7378 - val_acc: 0.1890\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.5172 - acc: 0.6092 - val_loss: 3.9834 - val_acc: 0.1807\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.8313 - acc: 0.0070 - val_loss: 4.7760 - val_acc: 0.0107\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.7287 - acc: 0.0223 - val_loss: 4.6963 - val_acc: 0.0317\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.4940 - acc: 0.0592 - val_loss: 4.4069 - val_acc: 0.0593\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.8325 - acc: 0.1422 - val_loss: 4.0558 - val_acc: 0.1123\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 2.9943 - acc: 0.2735 - val_loss: 3.7619 - val_acc: 0.1477\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 2.2472 - acc: 0.4265 - val_loss: 3.7977 - val_acc: 0.1760\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 1.5190 - acc: 0.6060 - val_loss: 3.9856 - val_acc: 0.1810\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8208 - acc: 0.0073 - val_loss: 4.7696 - val_acc: 0.0160\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6805 - acc: 0.0312 - val_loss: 4.6519 - val_acc: 0.0380\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.2777 - acc: 0.0837 - val_loss: 4.2381 - val_acc: 0.0897\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.4471 - acc: 0.1912 - val_loss: 3.7228 - val_acc: 0.1480\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.6733 - acc: 0.3455 - val_loss: 3.6780 - val_acc: 0.1640\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.9311 - acc: 0.5007 - val_loss: 3.8144 - val_acc: 0.1763\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.1988 - acc: 0.6748 - val_loss: 4.2336 - val_acc: 0.1923\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8285 - acc: 0.0100 - val_loss: 4.7794 - val_acc: 0.0110\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.7225 - acc: 0.0203 - val_loss: 4.7113 - val_acc: 0.0260\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.4840 - acc: 0.0632 - val_loss: 4.5332 - val_acc: 0.0497\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.8456 - acc: 0.1408 - val_loss: 4.0387 - val_acc: 0.1063\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 3.0430 - acc: 0.2762 - val_loss: 3.6760 - val_acc: 0.1480\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.2885 - acc: 0.4283 - val_loss: 3.5878 - val_acc: 0.1700\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.5740 - acc: 0.5905 - val_loss: 4.1740 - val_acc: 0.1767\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 0.9192 - acc: 0.7477 - val_loss: 4.3036 - val_acc: 0.1883\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.8232 - acc: 0.0080 - val_loss: 4.7689 - val_acc: 0.0190\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.6858 - acc: 0.0303 - val_loss: 4.6513 - val_acc: 0.0410\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.2995 - acc: 0.0852 - val_loss: 4.2298 - val_acc: 0.0800\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 3.5095 - acc: 0.1915 - val_loss: 3.8462 - val_acc: 0.1333\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 2.7306 - acc: 0.3290 - val_loss: 3.6587 - val_acc: 0.1573\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 2.0641 - acc: 0.4702 - val_loss: 3.5862 - val_acc: 0.1773\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 1.3788 - acc: 0.6310 - val_loss: 3.9210 - val_acc: 0.1847\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 0.7654 - acc: 0.7865 - val_loss: 4.4003 - val_acc: 0.1873\n",
      "4222/4222 [==============================] - 7s 2ms/step\n",
      "Train on 6000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.8292 - acc: 0.0075 - val_loss: 4.7834 - val_acc: 0.0163\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.7429 - acc: 0.0217 - val_loss: 4.7356 - val_acc: 0.0227\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 29s 5ms/step - loss: 4.5644 - acc: 0.0502 - val_loss: 4.5482 - val_acc: 0.0520\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 4.0204 - acc: 0.1107 - val_loss: 4.2370 - val_acc: 0.0973\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 3.2395 - acc: 0.2392 - val_loss: 3.7597 - val_acc: 0.1453\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 2.4441 - acc: 0.3883 - val_loss: 3.7483 - val_acc: 0.1697\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 1.7611 - acc: 0.5507 - val_loss: 3.8367 - val_acc: 0.1840\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 1.0296 - acc: 0.7268 - val_loss: 4.1806 - val_acc: 0.1800\n",
      "4222/4222 [==============================] - 7s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "for fov in FOV_VALS:\n",
    "    train_dir = os.path.join(PARENT_DIR, 'train', str(fov))\n",
    "    \n",
    "    for high_low in ['high', 'low']:\n",
    "        os.chdir(train_dir)\n",
    "        if high_low not in os.listdir():\n",
    "            os.mkdir(high_low)\n",
    "        train_x = np.load(high_low + '-coverage-images.npy') / 255.\n",
    "        train_y = np.load(high_low + '-coverage-labels.npy')\n",
    "        \n",
    "        train_ind = np.where(np.isin(train_y, dogs_subset))[0]\n",
    "        train_x = train_x[train_ind, :, :, :]\n",
    "        train_y = train_y[train_ind]\n",
    "        \n",
    "        y_train = np.asarray([y_dict[x] for x in train_y])\n",
    "        y_train = np_utils.to_categorical(y_train)\n",
    "        \n",
    "        os.chdir(high_low)\n",
    "        \n",
    "        for i in range(RUNS):\n",
    "            if str(i) not in os.listdir():\n",
    "                os.mkdir(str(i))\n",
    "            \n",
    "            vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=INPUT_SHAPE)\n",
    "            model = models.Sequential()\n",
    "            model.add(vgg_conv)\n",
    "            model.add(layers.Flatten())\n",
    "            model.add(layers.Dense(4096, activation='relu'))\n",
    "            model.add(layers.Dense(4096, activation='relu'))\n",
    "            model.add(layers.Dense(SUBSET_SIZE, activation='softmax'))\n",
    "            model.compile(loss='categorical_crossentropy', \n",
    "                          optimizer=optimizers.SGD(lr=.001, momentum=.9), \n",
    "                          metrics=['acc'])\n",
    "            \n",
    "        \n",
    "            tensorboard = TensorBoard(log_dir=os.path.join(train_dir, high_low, str(i)))\n",
    "            checkpoint = ModelCheckpoint(os.path.join(train_dir, high_low, str(i)) + '/model-{epoch:04d}.hdf5', \n",
    "                                         save_weights_only=False,\n",
    "                                         save_best_only=False,\n",
    "                                         mode='auto', \n",
    "                                         period=1)\n",
    "            earlystopping = EarlyStopping(monitor='val_loss', mode='min', patience=2)\n",
    "            \n",
    "            model.fit(train_x, y_train, \n",
    "                      validation_data=(val_x, y_val), \n",
    "                      epochs=100, \n",
    "                      batch_size=BATCH_SIZE, \n",
    "                      verbose=1,\n",
    "                      shuffle=True, \n",
    "                      callbacks=[tensorboard, checkpoint, earlystopping])\n",
    "            loss, accuracy = model.evaluate(test_x, y_test)\n",
    "            losses.append(loss)\n",
    "            accuracies.append(accuracy)\n",
    "            \n",
    "            K.clear_session()\n",
    "            del model\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fov_vals = np.repeat(FOV_VALS, [int(i) for i in np.ones(len(FOV_VALS)) * 2 * RUNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# highlow = np.tile(['high', 'high', 'high', 'high', 'low', 'low', 'low', 'low'], len(FOV_VALS))\n",
    "highlow = np.tile(np.concatenate([np.repeat('high', RUNS), \n",
    "                                  np.repeat('low', RUNS)]), \n",
    "                  len(FOV_VALS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({'fov': fov_vals, \n",
    "                           'size': highlow, \n",
    "                           'loss': losses, \n",
    "                           'accuracy': accuracies})\n",
    "results_df.to_csv('/media/johnkoo/data/stanford-dogs-dataset/results-size.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f3600068f98>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(x='fov', y='accuracy', hue='size', data=results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f35d8270c50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.pointplot(x='fov', y='accuracy', hue='size', data=results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fov</th>\n",
       "      <th>size</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>high</td>\n",
       "      <td>5.003292</td>\n",
       "      <td>0.023449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>high</td>\n",
       "      <td>5.036950</td>\n",
       "      <td>0.016106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>high</td>\n",
       "      <td>5.079213</td>\n",
       "      <td>0.015632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>high</td>\n",
       "      <td>5.006257</td>\n",
       "      <td>0.016817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>high</td>\n",
       "      <td>4.907089</td>\n",
       "      <td>0.024870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>high</td>\n",
       "      <td>4.975314</td>\n",
       "      <td>0.018475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30</td>\n",
       "      <td>high</td>\n",
       "      <td>5.344935</td>\n",
       "      <td>0.022501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30</td>\n",
       "      <td>high</td>\n",
       "      <td>4.890185</td>\n",
       "      <td>0.027712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30</td>\n",
       "      <td>low</td>\n",
       "      <td>5.341550</td>\n",
       "      <td>0.112743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30</td>\n",
       "      <td>low</td>\n",
       "      <td>4.407068</td>\n",
       "      <td>0.104453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>30</td>\n",
       "      <td>low</td>\n",
       "      <td>5.075404</td>\n",
       "      <td>0.110374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>30</td>\n",
       "      <td>low</td>\n",
       "      <td>4.450437</td>\n",
       "      <td>0.089768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>30</td>\n",
       "      <td>low</td>\n",
       "      <td>4.541444</td>\n",
       "      <td>0.089768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>30</td>\n",
       "      <td>low</td>\n",
       "      <td>4.582112</td>\n",
       "      <td>0.091426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>30</td>\n",
       "      <td>low</td>\n",
       "      <td>4.461005</td>\n",
       "      <td>0.112743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>30</td>\n",
       "      <td>low</td>\n",
       "      <td>4.648943</td>\n",
       "      <td>0.109901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>40</td>\n",
       "      <td>high</td>\n",
       "      <td>4.704724</td>\n",
       "      <td>0.056371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>40</td>\n",
       "      <td>high</td>\n",
       "      <td>4.953090</td>\n",
       "      <td>0.074372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>40</td>\n",
       "      <td>high</td>\n",
       "      <td>4.850923</td>\n",
       "      <td>0.033160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>40</td>\n",
       "      <td>high</td>\n",
       "      <td>5.050182</td>\n",
       "      <td>0.067504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>40</td>\n",
       "      <td>high</td>\n",
       "      <td>4.788784</td>\n",
       "      <td>0.056135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>40</td>\n",
       "      <td>high</td>\n",
       "      <td>4.853013</td>\n",
       "      <td>0.064188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>40</td>\n",
       "      <td>high</td>\n",
       "      <td>4.779680</td>\n",
       "      <td>0.048318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>40</td>\n",
       "      <td>high</td>\n",
       "      <td>4.964642</td>\n",
       "      <td>0.092610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>40</td>\n",
       "      <td>low</td>\n",
       "      <td>4.006092</td>\n",
       "      <td>0.191142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>40</td>\n",
       "      <td>low</td>\n",
       "      <td>4.823670</td>\n",
       "      <td>0.140928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>40</td>\n",
       "      <td>low</td>\n",
       "      <td>4.440682</td>\n",
       "      <td>0.160824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>40</td>\n",
       "      <td>low</td>\n",
       "      <td>3.844164</td>\n",
       "      <td>0.170772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>40</td>\n",
       "      <td>low</td>\n",
       "      <td>3.872032</td>\n",
       "      <td>0.192800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>40</td>\n",
       "      <td>low</td>\n",
       "      <td>4.889740</td>\n",
       "      <td>0.164140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>60</td>\n",
       "      <td>high</td>\n",
       "      <td>3.653150</td>\n",
       "      <td>0.251303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>60</td>\n",
       "      <td>high</td>\n",
       "      <td>4.538068</td>\n",
       "      <td>0.207011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>60</td>\n",
       "      <td>high</td>\n",
       "      <td>3.928028</td>\n",
       "      <td>0.193036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>60</td>\n",
       "      <td>high</td>\n",
       "      <td>4.186113</td>\n",
       "      <td>0.239697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>60</td>\n",
       "      <td>high</td>\n",
       "      <td>3.969895</td>\n",
       "      <td>0.211985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>60</td>\n",
       "      <td>high</td>\n",
       "      <td>3.938422</td>\n",
       "      <td>0.213880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>60</td>\n",
       "      <td>low</td>\n",
       "      <td>4.082302</td>\n",
       "      <td>0.210090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>60</td>\n",
       "      <td>low</td>\n",
       "      <td>3.746741</td>\n",
       "      <td>0.233775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>60</td>\n",
       "      <td>low</td>\n",
       "      <td>4.143200</td>\n",
       "      <td>0.237802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>60</td>\n",
       "      <td>low</td>\n",
       "      <td>4.359475</td>\n",
       "      <td>0.231170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>60</td>\n",
       "      <td>low</td>\n",
       "      <td>4.322332</td>\n",
       "      <td>0.199195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>60</td>\n",
       "      <td>low</td>\n",
       "      <td>3.848495</td>\n",
       "      <td>0.206537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>60</td>\n",
       "      <td>low</td>\n",
       "      <td>3.982371</td>\n",
       "      <td>0.218617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>60</td>\n",
       "      <td>low</td>\n",
       "      <td>4.191160</td>\n",
       "      <td>0.223354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>70</td>\n",
       "      <td>high</td>\n",
       "      <td>3.790321</td>\n",
       "      <td>0.267172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>70</td>\n",
       "      <td>high</td>\n",
       "      <td>4.119836</td>\n",
       "      <td>0.234960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>70</td>\n",
       "      <td>high</td>\n",
       "      <td>4.102984</td>\n",
       "      <td>0.229749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>70</td>\n",
       "      <td>high</td>\n",
       "      <td>3.550100</td>\n",
       "      <td>0.244671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>70</td>\n",
       "      <td>high</td>\n",
       "      <td>3.832603</td>\n",
       "      <td>0.238513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>70</td>\n",
       "      <td>high</td>\n",
       "      <td>3.838619</td>\n",
       "      <td>0.228565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>high</td>\n",
       "      <td>3.345831</td>\n",
       "      <td>0.296305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>70</td>\n",
       "      <td>high</td>\n",
       "      <td>4.065925</td>\n",
       "      <td>0.274041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>70</td>\n",
       "      <td>low</td>\n",
       "      <td>4.118331</td>\n",
       "      <td>0.159403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>70</td>\n",
       "      <td>low</td>\n",
       "      <td>4.144480</td>\n",
       "      <td>0.167693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>70</td>\n",
       "      <td>low</td>\n",
       "      <td>3.957279</td>\n",
       "      <td>0.181194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>70</td>\n",
       "      <td>low</td>\n",
       "      <td>3.957727</td>\n",
       "      <td>0.184510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>70</td>\n",
       "      <td>low</td>\n",
       "      <td>4.252598</td>\n",
       "      <td>0.189484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>70</td>\n",
       "      <td>low</td>\n",
       "      <td>4.223989</td>\n",
       "      <td>0.185220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>70</td>\n",
       "      <td>low</td>\n",
       "      <td>4.356078</td>\n",
       "      <td>0.184983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>70</td>\n",
       "      <td>low</td>\n",
       "      <td>4.192915</td>\n",
       "      <td>0.179773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    fov  size      loss  accuracy\n",
       "0    30  high  5.003292  0.023449\n",
       "1    30  high  5.036950  0.016106\n",
       "2    30  high  5.079213  0.015632\n",
       "3    30  high  5.006257  0.016817\n",
       "4    30  high  4.907089  0.024870\n",
       "5    30  high  4.975314  0.018475\n",
       "6    30  high  5.344935  0.022501\n",
       "7    30  high  4.890185  0.027712\n",
       "8    30   low  5.341550  0.112743\n",
       "9    30   low  4.407068  0.104453\n",
       "10   30   low  5.075404  0.110374\n",
       "11   30   low  4.450437  0.089768\n",
       "12   30   low  4.541444  0.089768\n",
       "13   30   low  4.582112  0.091426\n",
       "14   30   low  4.461005  0.112743\n",
       "15   30   low  4.648943  0.109901\n",
       "16   40  high  4.704724  0.056371\n",
       "17   40  high  4.953090  0.074372\n",
       "18   40  high  4.850923  0.033160\n",
       "19   40  high  5.050182  0.067504\n",
       "20   40  high  4.788784  0.056135\n",
       "21   40  high  4.853013  0.064188\n",
       "22   40  high  4.779680  0.048318\n",
       "23   40  high  4.964642  0.092610\n",
       "24   40   low  4.006092  0.191142\n",
       "25   40   low  4.823670  0.140928\n",
       "26   40   low  4.440682  0.160824\n",
       "27   40   low  3.844164  0.170772\n",
       "28   40   low  3.872032  0.192800\n",
       "29   40   low  4.889740  0.164140\n",
       "..  ...   ...       ...       ...\n",
       "50   60  high  3.653150  0.251303\n",
       "51   60  high  4.538068  0.207011\n",
       "52   60  high  3.928028  0.193036\n",
       "53   60  high  4.186113  0.239697\n",
       "54   60  high  3.969895  0.211985\n",
       "55   60  high  3.938422  0.213880\n",
       "56   60   low  4.082302  0.210090\n",
       "57   60   low  3.746741  0.233775\n",
       "58   60   low  4.143200  0.237802\n",
       "59   60   low  4.359475  0.231170\n",
       "60   60   low  4.322332  0.199195\n",
       "61   60   low  3.848495  0.206537\n",
       "62   60   low  3.982371  0.218617\n",
       "63   60   low  4.191160  0.223354\n",
       "64   70  high  3.790321  0.267172\n",
       "65   70  high  4.119836  0.234960\n",
       "66   70  high  4.102984  0.229749\n",
       "67   70  high  3.550100  0.244671\n",
       "68   70  high  3.832603  0.238513\n",
       "69   70  high  3.838619  0.228565\n",
       "70   70  high  3.345831  0.296305\n",
       "71   70  high  4.065925  0.274041\n",
       "72   70   low  4.118331  0.159403\n",
       "73   70   low  4.144480  0.167693\n",
       "74   70   low  3.957279  0.181194\n",
       "75   70   low  3.957727  0.184510\n",
       "76   70   low  4.252598  0.189484\n",
       "77   70   low  4.223989  0.185220\n",
       "78   70   low  4.356078  0.184983\n",
       "79   70   low  4.192915  0.179773\n",
       "\n",
       "[80 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
