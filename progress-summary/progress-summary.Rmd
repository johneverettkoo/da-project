---
title: Identifying Measures of Training Data Quality for Classifying Images
author:
  - David Crandall
  - John Koo
  - Michael Trosset
output: 
  pdf_document:
    citation_package: natbib
bibliography: progress-summary.bib
# output: html_document
# geometry: "left=1cm,right=1cm,top=1cm,bottom=1.5cm"
urlcolor: blue
header-includes:
- \usepackage{float}
- \usepackage{mathtools}
abstract: 'Convolutional neural networks (CNNs) are now widely utilized to fit highly accurate image classification models. However, in order to achieve these results, CNNs require vast amounts of training data, especially as the size of these networks grows in an effort to achieve increasingly better performance. In real-world applications, large amounts of training data are often difficult to obtain due to data collection and labelling limitations or difficult to work with due to computational limitations. Our work aims to address this problem by setting a training data "budget" by limiting the number of images allowed for model fitting and developing a method of choosing training images for the best model performance under this constraint. We explore measures of image quality and diversity to define a training data quality metric, inspired by image embedding methods and measures of dissimilarity and distance.'
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      comment = NA, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.pos = 'H', 
                      fig.align = 'center', 
                      fig.height = 2, 
                      fig.width = 4, 
                      fig.dpi = 100)
options(xtable.comment = FALSE, 
        xtable.table.placement = 'H')

library(ggplot2)
import::from(magrittr, `%>%`, `%<>%`)

# theme_set(ggthemes::theme_base())
theme_set(theme_bw())
z <- 1  # number of standard deviations for plotting
```

# Introduction

LeNet-5 \citep{Lecun98gradient-basedlearning} was the first widely recognized 
CNN architecture for image classification. Consisting of only seven layers,
three of which are convolutional, training this network on $32 \times 32$ 
greyscale images of handwritten digits involved fitting 60,000 model parameters. 
Over time, as larger datasets and more powerful computer hardware became 
available, CNN architectures grew deeper and more complicated (AlexNet
\citep{Krizhevsky:2012:ICD:2999134.2999257}, consists of 8 layers and 60M
parameters, and VGG16 \citep{simonyan2014deep}, consists of 41 layers and 
138M parameters). Due to the massive number of training parameters, these deep 
models require large amounts of data to prevent overfitting. In fact, it has 
been observed that performance gains can continue to scale with the training 
set size, even into the billions \citep{mahajan2018exploring}. 

However, obtaining large datasets for deep models is not always feasible. 
Manually labelling thousands or even millions of images can be tedious, 
time-consuming, and expensive \citep{Christensen_2016}.
Some proposed and empirically verified solutions to the limited data problem
include using a smaller network with fewer parameters, starting with a
pre-trained model, and image augmentation \citep{perez2017effectiveness}. In 
this paper, we propose methods for selectively choosing training images under a 
set data "budget" and discuss various measures and statistics on training sets 
that correlate to model performance on a separate test set. 

# Previous Work

\citet{NIPS2018_7396} demonstrated that given a large pool of images 
and a fixed training set size, it is possible to tailor the training set for 
fitting a VGG16 network that results in better or worse performance on a 
separate test set. They then described two characteristics of the datasets that 
seemed to correspond to model performance: object size (how much of the image 
the object takes up) and diversity (after embedding the images in 
$\mathbb{R}^d$, how much space the training point cloud takes up). Their study 
showed that model performance correlated positively with object size in the 
training set, and training sets consisting of "diverse" images tended to 
outperform those consisting of "similar" images. 

## Replication Study

The data in the above study can be described as follows:

* Training images were sampled from the frames of first person video feeds. Each 
image contained one of 24 toys that the toddlers were playing with. The video 
was taken with a $70^\circ$ lens. Bounding boxes of the toys were drawn for 
each image to determine how much of the image each toy took up. 
* The training images were blurred around the object to simulate acuity. 
* Validation and testing sets consisted of artificially generated images of the 
24 toys. 

The size experiment was as follows:

* A training set was selected of the "largest" objects (images in which the 
object we wish to classify took up more of the image). Another training set 
was selected of the "smallest" objects (images in which the object took up 
less of the image). 
* The images were cropped to simulate different fields of view between 
$30^\circ$ and the original $70^\circ$. 
* For each object size training subset and field of view, a VGG16 network 
pretrained on the ImageNet dataset was fit to these images using a fixed set of 
hyperparameters. Then performance was measured on the test set. This was 
repeated 10 times to obtain interval estimates for performance. 

The results of this experiment suggested that using a training set of "larger" 
images results in better model performance than using a training set of 
"smaller" images. In addition, decreasing the field of view (i.e., cropping 
into the object of interest) of the training images resulted in progressively 
better performance. This suggests that in order to obtain the best training set,
the objects we wish to classify must be prominently displayed in the images.

The diversity experiment was as follows:

* The training images were embedded into high dimensional Euclidean space 
using GIST features \citep{Torralba:2003:CPO:644361.644382}. 
* Points were sampled from this embedding using a greedy algorithm to maximize 
the distance between the points. The corresponding images became the "diverse" 
training set while the reamining images became the "similar" training set. 
* VGG16 networks pretrained on the ImageNet dataset were fit to the two 
training sets, using the same fixed set of hyperparameters as before. This was 
repeated 10 times. Performance was measured on the test set. Field of view 
was also adjusted incrementally as in the previous experiment.

The results of the diversity experiment suggest that the "diverse" training set 
results in a better performing model than the "similar" training set. As before,
decreasing the field of view improved performance.

### Stanford Dogs Dataset

```{r}
dogs.diversity.path <- 
  '~/dev/da-project/misc/stanford-dogs/results-diversity.csv'
dogs.size.path <- 
  '~/dev/da-project/misc/stanford-dogs/results-size.csv'

dogs.diversity.df <- readr::read_csv(dogs.diversity.path)
dogs.size.df <- readr::read_csv(dogs.size.path)
```

In order to replicate the study as closely as possible with a different dataset,
we needed an image classification dataset consisting of only one object/class 
per image and yet also containing either bounding box or segmentation mask 
information for each image, in order to both measure the object size in the 
image as well as zoom into the object to simulate different fields of view. One
dataset that contains this information is the Stanford Dogs dataset 
\citep{KhoslaYaoJayadevaprakashFeiFei_FGVC2011}, consisting of around 20,000 
images of 120 different dog breeds. We assumed that the original images were 
taken with a $70^\circ$ field of view. Some of the images were of multiple 
dogs, and these images were discarded. For each breed, 100 images were randomly 
selected as the training set, 25 images were randomly selected as the 
validation set, and the rest were set aside for testing. We did not blur these 
images.

#### Size Experiment

Using the bounding box information, we calculated the proportion of the image 
that the dogs took up. Then for each breed, we split the 100 training images 
so that the "large" dataset contains images in which the dogs take up more of 
the image and the "small" dataset contains images in which the dogs take up 
less of the image. We also incrementally zoomed into the center of the bounding 
boxes to simulate lower fields of view.

The results (Figure \ref{fig:dogs_size}) suggest that when using the unscaled 
images ($70^\circ$ field of view), we obtain better model performance when 
training on images in which the dogs take up more of the image. This is 
consistent with the results of the previous study. However, in our results, 
we do not get progressively better performance as we decrease the field of 
view. Visual inspection of the images shows that the dogs already take up most 
of the image in many cases, compared to the toys taking up relatively little of
the images in the toys dataset (median bounding box proportion of around 50\% 
for the dogs dataset vs. around 10\% for the toys dataset). As we crop the 
images to simulate lower fields of view, in some cases, we end up cropping so 
much that we are left with just a patch of fur in our image. We also suspect 
that since in the toys dataset, the validation and testing images prominently 
display the toys, compared to the training dataset where the toys take up 
relatively little of the images, cropping in makes the training set look more 
like the testing set. No such disparity exists in the training vs. validation 
vs. testing sets for the dogs datasets. However, the fact that the "large" 
dataset outperformed the "small" dataset using the uncropped data suggests that 
there may be some relationship between model performance and object size.

```{r dogs_size, fig.cap = '"Size" experiment on the Stanford Dogs dataset.'}
dogs.size.df %>% 
  dplyr::mutate(size = ifelse(size == 'high', 'large', 'small')) %>% 
  dplyr::group_by(fov, size) %>% 
  dplyr::summarise(mean.acc = mean(accuracy),
                   sd.acc = sd(accuracy)) %>% 
  ggplot() + 
  scale_colour_brewer(palette = 'Set1') + 
  geom_line(aes(x = fov, y = mean.acc, colour = size)) + 
  geom_errorbar(aes(x = fov, 
                    ymin = mean.acc - z * sd.acc, 
                    ymax = mean.acc + z * sd.acc, 
                    colour = size),
                width = 1) + 
  labs(x = 'field of view', 
       y = 'test accuracy', 
       colour = 'train set type')
```

#### Diversity Experiment

As in the previous study, we embedded the images using GIST features and for 
each breed, sampled 50 points to maximize Euclidean distance, and set aside 
the corresponding images as the "diverse" training set. The reamining 50 images 
for each breed were set aside as the "similar" training set.

As in the size experiment, decreasing the field of view reduced model 
performance. There was no significant difference in model performance between 
the "diverse" and "similar" training sets, although perhaps we would've 
observed a more pronounced difference if we increased the number of repetitions
(Fig \ref{fig:dogs_diverse}).

```{r dogs_diverse, fig.cap = '"Diversity" experiment on the Stanford Dogs dataset.'}
dogs.diversity.df %>% 
  dplyr::group_by(fov, train_set_type) %>% 
  dplyr::summarise(mean.acc = mean(accuracy),
                   sd.acc = sd(accuracy)) %>% 
  ggplot() + 
  scale_colour_brewer(palette = 'Set1') + 
  geom_line(aes(x = fov, y = mean.acc, colour = train_set_type)) + 
  geom_errorbar(aes(x = fov, 
                    ymin = mean.acc - z * sd.acc, 
                    ymax = mean.acc + z * sd.acc, 
                    colour = train_set_type),
                width = 1) + 
  labs(x = 'field of view', 
       y = 'test accuracy', 
       colour = 'train set type')
```

### CIFAR-10 and MNIST Datasets

Focusing just on training set diversity and ignoring object size and field of 
view allows us to expand this study to more datasets, as we no longer require 
bounding boxes or segmentation masks. Two datasets commonly used for image 
classification experiments are the CIFAR-10 
\citep{Krizhevsky09learningmultiple} and MNIST \citep{lecun2010mnist} datasets.
Each consist of 10 object classes. We repeated the diversity experiment on 
each dataset, this time varying the training set size instead of the field of 
view.

```{r}
cifar10.path <- '~/dev/da-project/cifar-results.csv'
mnist.path <- '~/dev/da-project/mnist-results.csv'
cifar10.small.path <- '~/dev/da-project/cifar-results-small-data.csv'
mnist.small.path <- '~/dev/da-project/mnist-results-small-data.csv'

cifar.df <- readr::read_csv(cifar10.path)
mnist.df <- readr::read_csv(mnist.path)
cifar.small.df <- readr::read_csv(cifar10.small.path)
mnist.small.df <- readr::read_csv(mnist.small.path)
```

```{r mnist, fig.width = 5, fig.cap = 'Diversity experiment results on MNIST data.'}
mnist.df %>% 
  dplyr::group_by(train_size, train_set_type) %>% 
  dplyr::summarise(mean.acc = mean(accuracy),
                   sd.acc = sd(accuracy)) %>% 
  ggplot() + 
  # scale_y_log10() + 
  # scale_x_log10() + 
  scale_colour_brewer(palette = 'Set1') + 
  geom_line(aes(x = train_size, y = mean.acc, colour = train_set_type)) + 
  geom_errorbar(aes(x = train_size, 
                    ymin = mean.acc - z * sd.acc, 
                    ymax = mean.acc + z * sd.acc, 
                    colour = train_set_type),
                width = .03) + 
  labs(x = 'training set size per class', 
       y = 'test accuracy', 
       colour = 'train set type') + 
  scale_x_continuous(breaks = unique(mnist.df$train_size), 
                     trans = 'log10')
```

```{r cifar10, fig.width = 5, fig.cap = 'Diversity experiment results on CIFAR-10 data.'}
cifar.df %>% 
  # dplyr::filter(accuracy > .1) %>% 
  dplyr::group_by(train_size, train_set_type) %>% 
  dplyr::summarise(mean.acc = mean(accuracy),
                   sd.acc = sd(accuracy)) %>% 
  ggplot() + 
  # scale_y_log10() + 
  # scale_x_log10() +
  scale_colour_brewer(palette = 'Set1') + 
  geom_line(aes(x = train_size, y = mean.acc, colour = train_set_type)) + 
  geom_errorbar(aes(x = train_size, 
                    ymin = mean.acc - z * sd.acc, 
                    ymax = mean.acc + z * sd.acc, 
                    colour = train_set_type),
                width = .03) + 
  labs(x = 'training set size per class', 
       y = 'test accuracy', 
       colour = 'train set type') + 
  scale_x_continuous(breaks = unique(cifar.df$train_size), 
                     trans = 'log10')
```

Here, our results suggest no significant relationship between image diversity
and model performance (Fig. \ref{fig:mnist}, \ref{fig:cifar10}). However, this 
may perhaps be due to the fact that this classification task is easier. In 
particular, once we reach ~100 images per class in the MNIST data, all 
training sets attain ~95% accuracy. We tried repeating this experiment with 
very small training samples (Fig. \ref{fig:mnist_small}, \ref{fig:cifar_small}). 
However, again, we see no significant difference in the three training data 
subsets.

```{r mnist_small, fig.cap = 'Diversity experiment results on small MNIST data.'}
mnist.small.df %>% 
  # dplyr::arrange(train_size, train_set_type, -accuracy) %>%
  # dplyr::group_by(train_size, train_set_type) %>%
  # dplyr::top_frac(.1) %>%
  dplyr::group_by(train_size, train_set_type) %>% 
  dplyr::summarise(mean.acc = mean(accuracy),
                   sd.acc = sd(accuracy)) %>% 
  ggplot() + 
  scale_colour_brewer(palette = 'Set1') + 
  geom_line(aes(x = train_size, y = mean.acc, colour = train_set_type)) + 
  geom_errorbar(aes(x = train_size, 
                    ymin = mean.acc - z * sd.acc, 
                    ymax = mean.acc + z * sd.acc, 
                    colour = train_set_type),
                width = 2) + 
  labs(x = 'training set size per class', 
       y = 'test accuracy', 
       colour = 'train set type') + 
  scale_x_continuous(breaks = unique(mnist.small.df$train_size))
```

```{r cifar_small, fig.cap = 'Diversity experiment results on small CIFAR-10 data.'}
cifar.small.df %>% 
  # dplyr::arrange(train_size, train_set_type, -accuracy) %>%
  # dplyr::group_by(train_size, train_set_type) %>%
  # dplyr::top_frac(.5) %>%
  dplyr::group_by(train_size, train_set_type) %>% 
  dplyr::summarise(mean.acc = mean(accuracy),
                   sd.acc = sd(accuracy)) %>% 
  ggplot() + 
  scale_colour_brewer(palette = 'Set1') + 
  geom_line(aes(x = train_size, y = mean.acc, colour = train_set_type)) + 
  geom_errorbar(aes(x = train_size, 
                    ymin = mean.acc - z * sd.acc, 
                    ymax = mean.acc + z * sd.acc, 
                    colour = train_set_type),
                width = 2) + 
  labs(x = 'training set size per class', 
       y = 'test accuracy', 
       colour = 'train set type') + 
  scale_x_continuous(breaks = unique(cifar.small.df$train_size))
```

### Alternative Methods for Training Set Selection

In the previous experiments, the training and validation sets were selected 
as follows:

1. A validation set was randomly sampled from the training set such that it 
contains 1,000 images of each class. 
2. Embed the remaining training images in $\mathbb{R}^{960}$ using GIST 
features.
3. From the reamining training images, sample $n_{sub}$ images from each class 
such that their GIST-embedded points are as far from each other as possible 
(in practice, we use a greedy algorithm for this since it is not possible to 
try all possible combinations of points, and this induces randomness in the 
sampling). The corresponding images comprise the "diverse" training set.
4. Sample $n_{sub}$ images from each class such that their GIST-embedded points 
are as close to each other as possible. This is done by selecting a random 
point and then selecting the $n_{sub} - 1$ nearest points. The corresponding 
images comprise the "similar" training set.
5. Sample $n_{sub}$ images from each class uniformly at random without
replacement.

Note that this method does not necessarily force the diverse and similar 
training sets to be disjoint (see Fig. \ref{fig:cloud}). In Fig.
\ref{fig:diverse_cats}, \ref{fig:similar_cats}, \ref{fig:random_cats}, 10 cat 
images were sampled from a pool of 5,000 using the diverse, similar, and random 
sampling methods respectively.

```{r cloud, out.width = '33%', fig.cap = 'Diverse (red) vs similar (blue) samples taken from a uniform point cloud. The two samples are not disjoint, and some of the blue points may be masking some of the red points.'}
knitr::include_graphics('~/dev/da-project/images/pointcloud.png')
```

```{r diverse_cats, out.width = '33%', fig.cap = 'Images of cats selected by the diverse sampling method.'}
knitr::include_graphics('~/dev/da-project/images/diverse-cats.png')
```

```{r similar_cats, out.width = '33%', fig.cap = 'Images of cats selected by the similar sampling method.'}
knitr::include_graphics('~/dev/da-project/images/similar-cats.png')
```

```{r random_cats, out.width = '33%', fig.cap = 'Images of cats selected by the random sampling method.'}
knitr::include_graphics('~/dev/da-project/images/random-cats.png')
```

We believe that this method is as close as possible to the diversity experiment
described by \citet{NIPS2018_7396} using our datasets. However, this method 
raises some problems. First, if we were to think of this method as 
intelligently selecting a small training sample, then the large and randomly
selected validation set seems out of place. Second, if we draw points from a 
$\mathbb{R}^d$ such that the points are as far from each other as possible and 
our sample size is $n \leq 2 d$, and we think of the original sample as a point
cloud, we would expect all of our points to lie on the edges of the point
cloud with no interior points. This may manifest itself in selecting 
"outlier" images, or images that are unlike the typical image of its class. 
We can address the first issue by selecting our validation set as follows: 
First, draw $2 n_{sub}$ images for both the diverse and similar training sets, 
then for each training set, set aside half for validation. This will be called 
the "validation from training" sampling method. We can address the 
second issue by first taking a random sample of size $2 n_{sub}$ and then 
dividing that sample into diverse and similar subsets by choosing the diverse 
subset according to maximal distance and setting the remainder as the similar 
subset. We will call this the "subset before sampling training" sampling method.

Another possible way to address the second issue is by projecting the embedded 
points into a lower dimensional space before sampling. 

***[insert table of different sampling methods here]***

```{r}
cifar10.val.from.train.path <- file.path('~/dev/da-project',
                                         'subset-validation-from-training',
                                         'cifar-constrained-val-results.csv')
mnist.val.from.train.path <- file.path('~/dev/da-project', 
                                       'subset-validation-from-training',
                                       'mnist-constrained-val-results.csv')
cifar10.val.from.train.df <- readr::read_csv(cifar10.val.from.train.path)
mnist.val.from.train.df <- readr::read_csv(mnist.val.from.train.path)
```

```{r mnist_val_constrained, fig.cap = 'Diversity experiment on MNIST data, using the "validation from training" sampling method.'}
mnist.val.from.train.df %>% 
  # dplyr::filter(accuracy > .15) %>% 
  dplyr::group_by(train_size, train_set_type) %>% 
  dplyr::summarise(mean.acc = mean(accuracy),
                   sd.acc = sd(accuracy)) %>% 
  ggplot() + 
  scale_x_log10() + 
  # scale_y_log10() + 
  geom_line(aes(x = train_size, y = mean.acc, colour = train_set_type)) + 
  scale_colour_brewer(palette = 'Set1') + 
  geom_errorbar(aes(x = train_size, 
                    ymin = mean.acc - z * sd.acc, 
                    ymax = mean.acc + z * sd.acc, 
                    colour = train_set_type)) + 
  labs(x = 'training set size per class', 
       y = 'test accuracy',
       colour = NULL)
```

```{r cifar10_val_constrained, fig.cap = 'Diversity experiment on CIFAR-10 data, using the "validation from training" sampling method.'}
cifar10.val.from.train.df %>% 
  # dplyr::filter(accuracy > .15) %>% 
  dplyr::group_by(train_size, train_set_type) %>% 
  dplyr::summarise(mean.acc = mean(accuracy),
                   sd.acc = sd(accuracy)) %>% 
  ggplot() + 
  scale_x_log10() + 
  # scale_y_log10() + 
  geom_line(aes(x = train_size, y = mean.acc, colour = train_set_type)) + 
  scale_colour_brewer(palette = 'Set1') + 
  geom_errorbar(aes(x = train_size, 
                    ymin = mean.acc - z * sd.acc, 
                    ymax = mean.acc + z * sd.acc, 
                    colour = train_set_type)) + 
  labs(x = 'training set size per class', 
       y = 'test accuracy',
       colour = NULL)
```

Figures \ref{fig:mnist_val_constrained} and \ref{fig:cifar10_val_constrained} 
suggest that the diverse sampling method results in higher test accuracy than 
the similar sampling method when the validation sets are sampled from the 
training sets. However, for either dataset, the randomly sampled training sets 
perform as well or outperform the diversely sampled training sets. This 
may be because when the validation sets are large random samples, the model is 
able to generalize despite sub-optimal training sets. We can also see that for 
very small training set sizes, models trained on diverse samples tend to 
perform worse than models trained on random samples, and as training set sizes 
increase, the performance of models trained on samples selected with either 
method begin to coincide. One explanation of this might be that the diverse 
sampling method tends to choose outliers or atypical images, and as the 
sample size increases, the diverse sampling method exhausts the set of outliers 
and begins to sample more typical images. Previous studies show that 
classification models tend to perform better when outliers or atypical 
observations are removed from the training set. 

```{r}
cifar10.train.from.subset.path <- file.path(
  '~/dev/da-project',
  'subset-data-before-sampling-training-sets', 
  'cifar10-subsampled-training-results.csv')
cifar10.train.from.subset.df <- readr::read_csv(cifar10.train.from.subset.path)
```

```{r cifar10_train_from_subset, fig.cap = 'Diversity experiment on CIFAR-10 data using the "subset before sampling training" sampling method.'}
cifar10.train.from.subset.df %>% 
  # dplyr::filter(accuracy > .15) %>% 
  dplyr::group_by(train_size, train_set_type) %>% 
  dplyr::summarise(mean.acc = mean(accuracy),
                   sd.acc = sd(accuracy)) %>% 
  ggplot() + 
  scale_x_log10() + 
  # scale_y_log10() + 
  geom_line(aes(x = train_size, y = mean.acc, colour = train_set_type)) + 
  scale_colour_brewer(palette = 'Set1') + 
  geom_errorbar(aes(x = train_size, 
                    ymin = mean.acc - z * sd.acc, 
                    ymax = mean.acc + z * sd.acc, 
                    colour = train_set_type)) + 
  labs(x = 'training set size per class', 
       y = 'test accuracy',
       colour = NULL)
```

Fig. \ref{fig:cifar10_train_from_subset} shows that the diverse, similar, and 
random training samples using the "subset before sampling training" sampling 
method results in no significant differences in model performance. 

## Other Related Work

\citet{wang2018data} proposed a two-round training approach to better fit 
CNNs on a subset of the training set. In their approach, A CNN was fit on a 
large training set, then for each image in the training set, an influence 
measure was computed over all images in the validation set. If the influence 
was negative, then that training image was discarded. The CNN was then fit 
again using the reduced training set, and the resulting model was observed to 
achieve a higher test accuracy than the original model trained prior to 
subsetting the training data. However, this still requies training on a large 
dataset, and their goal is to fine-tune a large training set by dropping "bad" 
observations rather than building an optimal small training set.

\citet{kaushal2019learning} also proposed an active learning approach to limit 
the size of training data. Their method is as follows: First, a small subset of 
a large training set is sampled, and a model is fit on the subset. Predictions 
are then made on the hold-out images in the training set based on this model. 
Finally, additional images are chosen based on the uncertainty of the model 
predictions, and these images are added to the training subset. This is then 
repeated over multiple rounds, increasing the size of the training data over 
each round. The study demonstrates that this method outperforms training on 
randomly sampled subsets of the same size. 

\citet{Ferreira2016UnsupervisedES} proposed a maximum entropy based sampling 
method for selecting training data, given that the inputs are of the form 
$x_i \in \mathbb{R}^d$.

\citet{Wilson1972AsymptoticPO} demonstrated that edited $k$-nearest neighbors 
classifiers outperform regular "unedited" $k$-nearest neighbors classifiers. 
This suggests that outliers in the training sample are detrimental to model 
training. 

Based on the literature, training set selection methods can be classified as 
denoising/filtering methods or as diversification methods. Denoising and 
filtering methods remove outliers or atypical observations, while 
diversification methods aim to make training observations as different from 
each other as possible. These two ideas appear to be at odds with one another. 
There also doesn't appear to be much literature on how to apply such methods on 
image data, as they assume that the data can be naturally represented in 
Euclidean space (i.e., as a data matrix). However, this does provide some sense 
of how we can "construct" a good training sample: Given a "good" embedding of 
images such that the embedding space can be separated by some set of manifolds 
into regions that correspond to each class, for each class, we should choose 
a training sample that fills up that class' region without crossing over into 
any other regions. 

One thing that is not clear is how this relates to the data collection process. 
Most previous studies sample from a pool of preexisting images, but in a 
more practical scenario, the data collection process would involve creating 
new images (e.g., by taking photographs). 

# New Proposed Methods

## Training Set Measures

One question we would like to answer is whether we can define a measure or 
statistic on a training sample that correlates with model performance. 

## Edited Training Data

The data editing method described by \cite{Wilson1972AsymptoticPO} is as 
follows:

1. Given a training set $\mathcal{X}$ such that 
$\mathcal{X} \subset \mathbb{R}^d$, $0 < i \leq n$, for each $i$, construct a
$k$NN model from $\mathcal{X} \setminus \{x_i\}$ to obtain $\hat{y}_i$.
2. For each $i$, if $\hat{y}_i \neq y_i$, set 
$\mathcal{X} \leftarrow \mathcal{X} \setminus \{x_i\}$, to obtain the edited 
training set $\tilde{\mathcal{X}}$.
3. Construct a $k$NN model from $\tilde{\mathcal{X}}$.

The results of this paper demonstrate that models constructed in this way tend 
to outperform models constructed using the entire training set. The intuition 
behind this result is that the training set editing method removes per-class 
outliers to construct smoother decision boundaries. 

The three training samples in this section are constructed as follows:

1. Draw a random sample from the training data.
2. Draw a "similar" sample from the training data.
3. Edit the training data using Wilson's data editing method, then draw a 
"diverse" sample from the edited training data.

The validation sets are subsetted from the three training samples after they 
are drawn (instead of drawing randomly from the original training set). 

The intuition here is that the diverse sampling method may favor drawing 
"outliers", but we still want to impose diversity on the training sample, so 
the hope is that the data editing method will remove those outliers and the 
diverse sampling method can draw from a "clean" pool of images.

In our data editing method, we first embedded the images using GIST features, 
then used principal component analysis to reduce the dimensionality (using 
enough components to obtain 80\% explained variance). Then we constructed a 
$5$-NN model on the projected embedding to determine which images to remove. 

Fig. \ref{fig:cifar10_edited} suggests that diverse sampling from an edited 
set results in worse model performance than just drawing a random sample. 
Fig. \ref{fig:edited_vs_unedited} shows the impact of editing before drawing a 
diverse sample.

```{r}
cifar10.edited.path <- file.path(
  '~/dev/da-project',
  'edited-training-data', 
  'cifar10-edited-training-results.csv')
cifar10.edited.df <- readr::read_csv(cifar10.edited.path)
```

```{r cifar10_edited, fig.cap = 'Diversity experiment on CIFAR-10 data, drawing from an edited dataset.'}
cifar10.edited.df %>% 
  # dplyr::filter(accuracy > .15) %>% 
  dplyr::group_by(train_size, train_set_type) %>% 
  dplyr::summarise(mean.acc = mean(accuracy),
                   sd.acc = sd(accuracy)) %>% 
  ggplot() + 
  scale_x_log10() + 
  # scale_y_log10() + 
  geom_line(aes(x = train_size, y = mean.acc, colour = train_set_type)) + 
  scale_colour_brewer(palette = 'Set1') + 
  geom_errorbar(aes(x = train_size, 
                    ymin = mean.acc - z * sd.acc, 
                    ymax = mean.acc + z * sd.acc, 
                    colour = train_set_type)) + 
  labs(x = 'training set size per class', 
       y = 'test accuracy',
       colour = NULL)
```

```{r edited_vs_unedited, fig.cap = 'Impact of editing the training data prior to drawing a "diverse" sample'}
cifar10.edited.df %>% 
  dplyr::filter(train_set_type == 'diverse') %>% 
  dplyr::inner_join(cifar10.val.from.train.df, 
                    by = c('train_size', 'train_set_type')) %>%
  dplyr::group_by(train_size) %>% 
  dplyr::summarise(mean.acc = mean(accuracy.y), 
                   mean.acc.edited = mean(accuracy.x),
                   sd.acc = sd(accuracy.y), 
                   sd.acc.edited = sd(accuracy.x)) %>% 
  ggplot() + 
  scale_x_log10() + 
  scale_colour_brewer(palette = 'Set1') + 
  geom_line(aes(x = train_size, y = mean.acc, colour = 'not edited')) + 
  geom_line(aes(x = train_size, y = mean.acc.edited, colour = 'edited')) + 
  geom_errorbar(aes(x = train_size, 
                    ymin = mean.acc - z * sd.acc,
                    ymax = mean.acc + z * sd.acc, 
                    colour = 'not edited')) + 
  geom_errorbar(aes(x = train_size, 
                    ymin = mean.acc.edited - z * sd.acc.edited, 
                    ymax = mean.acc.edited + z * sd.acc.edited, 
                    colour = 'edited')) + 
  labs(x = 'training set size per class', y = 'test accuracy', 
       colour = NULL)
```

**[also try random sampling from the edited data]**

## Clustering

We might be able to think of each class as having "subclasses". For example, 
one of the classes in the CIFAR-10 dataset is "dog", but we can further 
subdivide that into different breeds of dogs (or groups of breeds of dogs). It 
might be beneficial to have at least a couple representative images from each 
subclass. The sampling procedures tested in this section are described as 
follows: 

1. Start with an overall training set. Optionally, perform Wilson editing to 
remove outliers.
2. Draw a random sample ("random" set).
3. For each class, perform $k$-means clustering. Then draw a per-cluster 
random sample ("random by cluster" set) and a per-cluster diverse sample 
("diverse by cluster" set). 

Again, we split validation sets from each of the training sets sampled by this 
procedure. Here, we chose $k = 10$.

```{r}
cifar10.clustered.path <- 
  '~/dev/da-project/kmeans-sampling/cifar10-kmeans-results.csv'
cifar10.clustered.df <- readr::read_csv(cifar10.clustered.path)
cifar10.clustered.edited.path <- 
  '~/dev/da-project/kmeans-sampling/cifar10-kmeans-edited-results.csv'
cifar10.clustered.edited.df <- readr::read_csv(cifar10.clustered.edited.path)
```

```{r cifar10_clustered, fig.cap = 'Diversity experiment on CIFAR-10 data, drawing from clusters.', fig.width = 4.5}
cifar10.clustered.df %>% 
  # dplyr::filter(accuracy > .15) %>% 
  dplyr::group_by(train_size, train_set_type) %>% 
  dplyr::summarise(mean.acc = mean(accuracy),
                   sd.acc = sd(accuracy)) %>% 
  ggplot() + 
  # scale_x_log10() + 
  # scale_y_log10() + 
  geom_line(aes(x = train_size, y = mean.acc, colour = train_set_type)) + 
  scale_colour_brewer(palette = 'Set1') + 
  geom_errorbar(aes(x = train_size, 
                    ymin = mean.acc - z * sd.acc, 
                    ymax = mean.acc + z * sd.acc, 
                    colour = train_set_type)) + 
  labs(x = 'training set size per class', 
       y = 'test accuracy',
       colour = NULL)
```

```{r cifar10_clustered_edited, fig.cap = 'Diversity experiment on CIFAR-10 data, drawing from clusters. The overall training set was edited prior to sampling.'}
cifar10.clustered.edited.df %>% 
  # dplyr::filter(accuracy > .15) %>% 
  dplyr::group_by(train_size, train_set_type) %>% 
  dplyr::summarise(mean.acc = mean(accuracy),
                   sd.acc = sd(accuracy)) %>% 
  ggplot() + 
  # scale_x_log10() + 
  # scale_y_log10() + 
  geom_line(aes(x = train_size, y = mean.acc, colour = train_set_type)) + 
  scale_colour_brewer(palette = 'Set1') + 
  geom_errorbar(aes(x = train_size, 
                    ymin = mean.acc - z * sd.acc, 
                    ymax = mean.acc + z * sd.acc, 
                    colour = train_set_type)) + 
  labs(x = 'training set size per class', 
       y = 'test accuracy',
       colour = NULL)
```

* One cluster per image
* Using clustering to remove outliers

## Transfer Learning-Based Methods

* Using an embedding from the last layer of another CNN model

## Active Learning-Based Methods

* Pre-training on a small subset to obtain an embedding

## Methods Leveraging Extra Information

* CIFAR-100
* COIL-20 and COIL-100

## Variational Autoencoders

* How to map from embedding to image

# Conclusions and Next Steps

\newpage

# References