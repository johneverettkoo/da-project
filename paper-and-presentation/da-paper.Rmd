---
title: Training Set Selection for Image Classification
subtitle: Department of Statistics Data Analysis Qualifying Exam
author:
- David Crandall
- John Koo
- Michael Trosset
date: December 16, 2019
output: 
  pdf_document:
    number_sections: yes
    citation_package: natbib
    latex_engine: pdflatex
bibliography: da-paper.bib
# geometry: "left=1cm,right=1cm,top=1cm,bottom=1.5cm"
urlcolor: blue
header-includes:
- \usepackage{float}
- \usepackage{mathtools}
- \setcitestyle{numbers,square}
- \usepackage{caption}
- \captionsetup[figure]{font=scriptsize}
- \twocolumn
abstract: |
  Convolutional neural networks (CNNs) are now widely utilized to fit highly accurate image classification models. However, in order to achieve these results, CNNs require vast amounts of training data, especially as the size of these networks grows in an effort to achieve increasingly better performance. In real-world applications, large amounts of training data are often difficult to obtain due to data collection and labelling limitations or difficult to work with due to computational limitations. Expanding upon on previous work by Bambach, Crandall, Smith, and Yu \cite{NIPS2018_7396}, our work explores various methods for subsampling training images under a data budget for fitting an image classification model and compares the results against uniform random sampling. The intuition behind our methods is that we would like to sample a "diverse" training set while controlling for the probability of drawing atypical "outlier" images. While our results fail to outperform random sampling, we demonstrate the effects of fitting CNNs on training subsets drawn with varying degrees of diversity and outlyingness, which are measured using GIST-embedded distances.
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      comment = NA, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.pos = 'H', 
                      fig.align = 'center', 
                      fig.height = 3, 
                      fig.width = 5, 
                      fig.dpi = 100)
options(xtable.comment = FALSE, 
        xtable.table.placement = 'H')

library(ggplot2)
import::from(magrittr, `%>%`, `%<>%`)

# theme_set(ggthemes::theme_base())
theme_set(theme_bw())
z <- 1  # number of standard deviations for plotting
```

```{r two_cols, results='asis'}
# cat('\\twocolumn')
```

```{r data_ingest}
dogs.diversity.path <- 
  '~/dev/da-project/misc/stanford-dogs/results-diversity.csv'
dogs.size.path <- 
  '~/dev/da-project/misc/stanford-dogs/results-size.csv'
dogs.diversity.df <- readr::read_csv(dogs.diversity.path)
dogs.size.df <- readr::read_csv(dogs.size.path)


cifar10.path <- '~/dev/da-project/cifar-results.csv'
mnist.path <- '~/dev/da-project/mnist-results.csv'
cifar10.small.path <- '~/dev/da-project/cifar-results-small-data.csv'
mnist.small.path <- '~/dev/da-project/mnist-results-small-data.csv'
cifar10.df <- readr::read_csv(cifar10.path)
mnist.df <- readr::read_csv(mnist.path)
cifar10.small.df <- readr::read_csv(cifar10.small.path)
mnist.small.df <- readr::read_csv(mnist.small.path)
cifar10.df <- dplyr::bind_rows(cifar10.df, cifar10.small.df)
mnist.df <- dplyr::bind_rows(mnist.df, mnist.small.df)


cifar10.val.from.train.path <- file.path('~/dev/da-project',
                                         'subset-validation-from-training',
                                         'cifar10-constrained-val-results.csv')
mnist.val.from.train.path <- file.path('~/dev/da-project', 
                                       'subset-validation-from-training',
                                       'mnist-constrained-val-results.csv')
cifar10.val.from.train.df <- readr::read_csv(cifar10.val.from.train.path)
mnist.val.from.train.df <- readr::read_csv(mnist.val.from.train.path)

cifar10.edited.path <- file.path(
  '~/dev/da-project',
  'edited-training-data', 
  'cifar10-edited5nn-training-results.csv')
cifar10.edited.df <- readr::read_csv(cifar10.edited.path)

cifar10.clustered.path <- 
  '~/dev/da-project/kmeans-sampling/cifar10-kmeans-results.csv'
cifar10.clustered.df <- readr::read_csv(cifar10.clustered.path)
cifar10.clustered.edited.path <- 
  '~/dev/da-project/kmeans-sampling/cifar10-kmeans-edited-results.csv'
cifar10.clustered.edited.df <- readr::read_csv(cifar10.clustered.edited.path)

cifar10.transfer.path <- file.path(
  '~/dev/da-project/transfer-learning',
  'cifar10-transfer-learning-results.csv')
cifar10.transfer.edit.path <- file.path(
  '~/dev/da-project/transfer-learning',
  'cifar10-transfer-learning-edited-results.csv')
cifar10.transfer.cluster.path <- file.path(
  '~/dev/da-project/transfer-learning',
  'cifar10-transfer-learning-kmeans-results.csv')
cifar10.transfer.edit.cluster.path <- file.path(
  '~/dev/da-project/transfer-learning',
  'cifar10-transfer-learning-edited-kmeans-results.csv')
cifar10.translearn.df <- dplyr::bind_rows(
  readr::read_csv(cifar10.transfer.path),
  readr::read_csv(cifar10.transfer.edit.path) %>% 
    dplyr::filter(train_set_type != 'similar') %>% 
    dplyr::mutate(train_set_type = ifelse(train_set_type == 'random',
                                          'random', 
                                          'diverse with editing')),
  readr::read_csv(cifar10.transfer.cluster.path) %>% 
    dplyr::filter(train_set_type != 'diverse by cluster') %>% 
    dplyr::mutate(train_set_type = ifelse(train_set_type == 'random',
                                          'random',
                                          'by cluster')),
  readr::read_csv(cifar10.transfer.edit.cluster.path) %>% 
    dplyr::filter(train_set_type != 'diverse by cluster') %>% 
    dplyr::mutate(train_set_type = ifelse(train_set_type == 'random',
                                          'random',
                                          'edited and by cluster'))
) %>% 
  dplyr::filter(train_size <= 500)
```

# Introduction

LeNet-5, 1998 \cite{Lecun98gradient-basedlearning}, was the first widely
recognized CNN architecture for image classification. Consisting of only seven
layers,three of which are convolutional, training this network on 
greyscale images of handwritten digits involved fitting 60,000
model parameters. Over time, as larger datasets and more powerful computer
hardware became available, CNN architectures grew deeper and more complicated:
AlexNet, 2012 \cite{Krizhevsky:2012:ICD:2999134.2999257}, consists of 8 layers
and 60M parameters, and VGG16, 2015 \cite{simonyan2014deep}, consists of 41
layers and 138M parameters). Due to the massive number of training parameters,
these deep models require large amounts of data to prevent overfitting. In 
fact, it has been observed that performance gains can continue to scale with 
the training set size, even into the billions \cite{mahajan2018exploring}. 

However, obtaining large datasets for deep models is not always feasible. 
Manually labelling thousands or even millions of images can be tedious, 
time-consuming, and expensive \cite{Christensen_2016}.
Some proposed and empirically verified solutions to the limited data problem
include using a smaller network with fewer parameters, starting with a
pre-trained model, and increasing the effective training set size using image
augmentation \cite{perez2017effectiveness}. In this paper, we propose methods 
for selectively choosing training images under a set data "budget" and discuss
how they compare against uniform random sampling. 

The bulk of our work involves replicating a previous study \cite{NIPS2018_7396} 
using different datasets, checking if our results are consistent with theirs,
hypothesizing how these methods work, and expanding upon this work by 
attempting to come up with new methods based on some of the ideas put forth by 
the original study. The new methods we propose in this paper fail to outperform 
uniform random sampling.

# Previous Work

\citet{NIPS2018_7396} demonstrated that given a large pool of images 
and a fixed training set size, it is possible to tailor the training set for 
fitting a VGG16 network that results in better or worse performance on a 
separate test set. They then described two characteristics of the datasets that 
seemed to correspond to model performance: object size (how much of the image 
the object takes up) and diversity (after embedding the images in 
$\mathbb{R}^d$, how much space the training point cloud takes up). Their study 
showed that model performance correlated positively with object size in the 
training set, and training sets consisting of "diverse" images tended to 
outperform those consisting of "similar" images. 

## Replication Study

The data in the above study is as follows:

* Training images were sampled from the frames of first person video feeds, 
from the point of view of toddlers and parents playing with one of 24 toys. The 
video was taken with a $70^\circ$ lens. Bounding boxes of the toys were 
drawn for each image to determine the size and location of the toy. The images 
were blurred around the object using the bounding box information to simulate 
visual acuity.
* Validation and testing sets consisted of artificially generated images of the 
24 toys.

Two experiments were then performed on these data. 
Both experiments involved fitting VGG16 networks on a particular training set.

The size experiment can be described as follows: Frames were randomly sampled
from the video feeds and ranked according to object size (median of around 
10\%). These were then split into a training set of "big" objects and a 
training set of "small" objects. It was shown that the model fit on the big
objects outperformed the model fit on the small objects when comparing test
accuracies. The images were also cropped into the object to simulate varying
focal lengths from the original $70^\circ$ down to $30^\circ$ in increments of 
$10^\circ$, and the croppedimages outperformed the original images, further
supporting this result.

The diversity experiment can be described as follows: Again, frames were 
randomly sampled from video feeds. These frames were then embedded into 
Euclidean space using GIST features[^gist] 
\cite{Torralba:2003:CPO:644361.644382}. 
Three training subsets were sampled based on the GIST features: a "diverse" 
subset that maximizes pairwise distances, a "similar" subset that minimizes 
pairwise distances, and a "random" subset. Models fit on the random subset 
outperformed the models fit on the diverse subset which outperformed the models 
fit on the similar subset, using test accuracy to compare models. Images were
again cropped to simulate various focal lengths, and lower focal lengths again
resulted in better model performance. 

[^gist]: GIST features for our configuration are in $\mathbb{R}^{960}$.

We attempted to replicate[^tensorflow] this study using the Stanford Dogs 
dataset\cite{KhoslaYaoJayadevaprakashFeiFei_FGVC2011}, which consists of around 
20,000 images of 120 dog breeds. Most images contain one dog per image, and 
images that contain multiple dogs were discarded. For each breed, 100 images 
were randomly selected for the training set (which were further divided into 
50-50 big vs. small and diverse vs. similar training subsets based on the 
experiment), 25 images were randomly selected for the validation set, and the 
rest were set aside for testing. The validation set was used to determine when 
to stop training. No blurring was applied to these images. It is assumed that 
all images were taken with a $70^\circ$ lens. Each experiment was replicated 10 
times. 

[^tensorflow]: Models were fit using TensorFlow 
\cite{tensorflow2015-whitepaper} and Keras \cite{chollet2015keras}.

```{r dogs_size, fig.cap = '"Size" experiment on the Stanford Dogs dataset. Errorbars indicate $\\pm 1$ standard deviation from the mean of 10 repetitions.'}
dogs.size.df %>% 
  dplyr::mutate(size = ifelse(size == 'high', 'big', 'small')) %>% 
  dplyr::group_by(fov, size) %>% 
  dplyr::summarise(mean.acc = mean(accuracy),
                   sd.acc = sd(accuracy)) %>% 
  ggplot() + 
  scale_colour_brewer(palette = 'Set1') + 
  geom_line(aes(x = fov, y = mean.acc, colour = size)) + 
  geom_errorbar(aes(x = fov, 
                    ymin = mean.acc - z * sd.acc, 
                    ymax = mean.acc + z * sd.acc, 
                    colour = size),
                width = 1) + 
  labs(x = 'field of view', 
       y = 'test accuracy', 
       colour = NULL) + 
  theme(legend.key.width = unit(1, 'mm'),
        text = element_text(size = 9),
        legend.position = c(.8, .25),
        legend.background = element_blank())
```

```{r dogs_diverse, fig.cap = '"Diversity" experiment on the Stanford Dogs dataset. Errorbars indicate $\\pm 1$ standard deviation from the mean of 10 repetitions'}
dogs.diversity.df %>% 
  dplyr::group_by(fov, train_set_type) %>% 
  dplyr::summarise(mean.acc = mean(accuracy),
                   sd.acc = sd(accuracy)) %>% 
  ggplot() + 
  scale_colour_brewer(palette = 'Set1') + 
  geom_line(aes(x = fov, y = mean.acc, colour = train_set_type)) + 
  geom_errorbar(aes(x = fov, 
                    ymin = mean.acc - z * sd.acc, 
                    ymax = mean.acc + z * sd.acc, 
                    colour = train_set_type),
                width = 1) + 
  labs(x = 'field of view', 
       y = 'test accuracy', 
       colour = NULL) + 
  theme(legend.key.width = unit(1, 'mm'),
        text = element_text(size = 9),
        legend.position = c(.8, .25),
        legend.background = element_blank())
```

Fig. \ref{fig:dogs_size} shows that for the original, uncropped images (FoV =
$70^\circ$), models fit on the "big" subset tend to outperform models fit on 
the "small" subset. However, instead of increased performance with reduced 
field of view, we observed the opposite: as the images were cropped into the 
objects of interest, the resulting models performed worse, and this was 
especially true of the "big" subset. Closer inspection of the training images 
reveals that the median bounding box coverage was around 50\%, compared to 
10\% of the toys dataset, and cropping often resulted in cutting off parts of 
the object of interest (Fig. \ref{fig:crop_dog}).

Results of the diversity[^leargist] experiment (Fig. \ref{fig:dogs_diverse})
suggest that there may be some difference between the models fitted on 
"diverse" vs. "similar" training sets at the original focal lengths, but it is 
not clear whether this is a significant result. 

[^leargist]: Diversity was based on pairwise GIST features, which were 
extracted using a Python package created by \citet{lear_gist}.

```{r crop_dog, out.width = '10%', fig.cap = 'An image from the Stanford Dogs dataset cropped from an assumed $70^\\circ$ FoV to $30^\\circ$.'}
knitr::include_graphics('~/dev/da-project/images/cropped-dog.png')
```

```{r out.width = '50%'}
knitr::include_graphics('~/dev/da-project/images/diverse-cats.png')
```

```{r out.width = '50%'}
knitr::include_graphics('~/dev/da-project/images/similar-cats.png')
```

```{r cats, out.width = '50%', fig.cap = 'Images of cats from the CIFAR-10 dataset selected by the diverse (top), similar (middle), and random (bottom) sampling methods.'}
knitr::include_graphics('~/dev/da-project/images/random-cats.png')
```

```{r cifar10_diversity, fig.cap = 'Diversity experiment on CIFAR-10 data. The $x$-axis is on the log-scale. Errorbars indicate $\\pm 1$ standard deviation from the mean of 10 repetitions.'}
cifar10.val.from.train.df %>% 
  dplyr::group_by(train_size, train_set_type) %>% 
  dplyr::summarise(mean.acc = mean(accuracy),
                   sd.acc = sd(accuracy)) %>% 
  ggplot() + 
  scale_x_log10() + 
  # scale_y_log10() + 
  geom_line(aes(x = train_size, y = mean.acc, colour = train_set_type)) + 
  scale_colour_brewer(palette = 'Set1') + 
  geom_errorbar(aes(x = train_size, 
                    ymin = mean.acc - z * sd.acc, 
                    ymax = mean.acc + z * sd.acc, 
                    colour = train_set_type)) + 
  labs(x = 'training set size per class', 
       y = 'test accuracy',
       colour = NULL) + 
  theme(legend.key.width = unit(1, 'mm'),
        text = element_text(size = 9),
        legend.position = c(.85, .3),
        legend.background = element_blank())
```

Focusing just on diversity, we tried a similar experiment using the 
CIFAR-10 \cite{Krizhevsky09learningmultiple} dataset, which consists of 10 
object classes, each class consisting of 5,000 training images and 1,000 
test images. Instead of varying focal lengths (which is not possible with 
CIFAR-10 images), we varied the training set sizes. For a given training set 
size $n$, we sampled $2 n$ images from each class and set aside half for 
validation. The sampling was done according to the "diverse", "similar", and 
"random" sampling methods (see Fig. \ref{fig:cats}). Models are compared by 
their predictive accuracies on the held-out test set. The results show that as 
$n$ grows, model performance across all three sampling methods converge, which 
is expected as we approach $n \to$ original training set size. Disappointingly, 
the random sampling method tends to result in as good or better models than the 
diverse sampling method (Fig. \ref{fig:cifar10_diversity}. The similar sampling
method tends to result in the worst models of the three. One hypothesis on why 
the diverse sampling method underperforms is that it tends to select rather 
unique images that are unlike the others, and the model overfits to those 
images. 

## Other Related Work

\citet{wang2018data} proposed a two-round training approach to better fit 
CNNs on a subset of the training set. In their approach, a CNN was fit on a 
large training set, then for each image in the training set, an influence 
measure was computed over all images in a validation set. If the influence 
was negative, then that training image was discarded. The CNN was then fit 
again using the reduced training set, and the resulting model was observed to 
achieve a higher test accuracy than the original model trained prior to 
subsetting the training data. 

\citet{kaushal2019learning} also proposed an active learning approach to limit 
the size of training data. Their method is as follows: First, a small subset of 
a large training set is selected, and a model is fit on the subset. Predictions 
are then made on the unselected images in the training set based on this model. 
Images are chosen based on the uncertainty of the model prediction and added to
the training subset, and the model is refitted using the larger training subset.
This is then repeated over multiple rounds, increasing the size of the training
subset each round. The study demonstrates that this method outperforms training
on randomly sampled subsets of the same size. 

\citet{Ferreira2016UnsupervisedES} proposed a maximum entropy based subset selection method for selecting training data, given that the inputs are of the form $x_i \in \mathbb{R}^d$. Their method starts with a large training set to 
estimate the density of the feature space. 

\citet{Wilson1972AsymptoticPO} demonstrated that edited $k$-nearest neighbors 
classifiers outperform regular "unedited" $k$-nearest neighbors classifiers. 
The Wilson editing method is described as follows: Given a training set 
$X_1, ..., X_n \in \mathbb{R}^d$ with corresponding discrete labels
$Y_1, ..., Y_n \in \{1, 2, ..., q\}$, use leave-one-out cross-validated
$k$-nearest neighbors to determine $\hat{Y}_1, ..., \hat{Y}_n$. Discard 
$i \in \{1, ..., n\}$ where $Y_i \neq \hat{Y}_i$ to construct a reduced, edited 
training set. Finally, fit a new $k$-nearest neighbors model on the reduced
training set. Wilson's results show that the model fit on the edited data tend 
to outperform models fit on the entire training set, suggesting that outliers 
in the training set are detrimental to the resulting model performance. 

Based on the literature, training set selection methods can be classified as 
denoising/filtering methods or as diversification methods. Denoising and 
filtering methods remove outliers or atypical observations, while 
diversification methods aim to make training observations as different from 
each other as possible. These two ideas appear to be at odds with one another. 
There also doesn't appear to be as much literature on how to apply such methods
to image data, as they assume that the data can be naturally represented in 
Euclidean space (i.e., as a data matrix). However, this does provide some sense 
of how we can "construct" a good training sample: Given a "good" embedding of 
images such that the embedding space can be separated by some set of manifolds 
into regions that correspond to each class, for each class, we should choose 
a training sample that fills up that class' region (i.e., a "diverse" set) 
without crossing over into any other regions (i.e., no "outliers"). 

One thing that is not clear is how we can relate various training subset 
selection methods to the physical data collection process. Most previous 
studies sample from a pool of preexisting images, but in a more practical
scenario, the data collection process would involve creating new images (e.g., 
by taking photographs). It is also not clear how we can go from a point in the 
embedding space (GIST or otherwise) to an actual image.

# Methods and Results

## Data Editing

We used Wilson editing to remove "outliers" from the training set before 
subsampling using the diverse sampling method. The random and similar 
subsamples were drawn in their usual way. The idea behind this method is that
Wilson editing will "clean" the training set by remove outliers, and then we 
will try to construct a diverse training set from the cleaned data---hopefully
this will allow us to remove "bad" images while simultaneously spanning as
much of the image space as possible. $k$-nearest neighbors classification was 
performed using GIST features. We chose $k = 5$ based on cross-validated
accuracy[^cv]. The same VGG16 architecture was fit on these training subsets. 

Our results suggest that data editing prior to diverse sampling does improve 
model performance when training sample sizes are small (Fig. 
\ref{fig:edited_vs_unedited}). However, randomly sampled training subsets still
result in better models (Fig. \ref{fig:cifar10_diversity}).

[^cv]: To save on computational costs, we used 10-fold cross validation instead 
of the leave-one-out cross validation prescribed by Wilson. $5$-NN achieved 
approximately 60\% accuracy (compared to around 10\% accuracy when using just 
pixels as features).

```{r cifar10_edited, fig.cap = 'Diversity experiment on CIFAR-10 data, drawing from an edited dataset. The $x$-axis is on the log-scale. Errorbars indicate $\\pm 1$ standard deviation from the mean of 10 repetitions.'}
cifar10.edited.df %>% 
  dplyr::group_by(train_size, train_set_type) %>% 
  dplyr::summarise(mean.acc = mean(accuracy),
                   sd.acc = sd(accuracy)) %>% 
  ggplot() + 
  scale_x_log10() + 
  # scale_y_log10() + 
  geom_line(aes(x = train_size, y = mean.acc, colour = train_set_type)) + 
  scale_colour_brewer(palette = 'Set1') + 
  geom_errorbar(aes(x = train_size, 
                    ymin = mean.acc - z * sd.acc, 
                    ymax = mean.acc + z * sd.acc, 
                    colour = train_set_type)) + 
  labs(x = 'training set size per class', 
       y = 'test accuracy',
       colour = NULL) + 
  theme(legend.key.width = unit(1, 'mm'),
        text = element_text(size = 9),
        legend.position = c(.85, .3),
        legend.background = element_blank())
```

```{r edited_vs_unedited, fig.cap = 'Impact of editing the training data prior to drawing a "diverse" sample on the CIFAR-10 dataset. The $x$-axis is on the log-scale. Errorbars indicate $\\pm 1$ standard deviation from the mean of 10 repetitions.'}
cifar10.edited.df %>% 
  dplyr::filter(train_set_type == 'diverse') %>% 
  dplyr::inner_join(cifar10.val.from.train.df, 
                    by = c('train_size', 'train_set_type')) %>%
  dplyr::group_by(train_size) %>% 
  dplyr::summarise(mean.acc = mean(accuracy.y), 
                   mean.acc.edited = mean(accuracy.x),
                   sd.acc = sd(accuracy.y), 
                   sd.acc.edited = sd(accuracy.x)) %>% 
  ggplot() + 
  scale_x_log10() + 
  scale_colour_brewer(palette = 'Set1') + 
  geom_line(aes(x = train_size, y = mean.acc, colour = 'not edited')) + 
  geom_line(aes(x = train_size, y = mean.acc.edited, colour = 'edited')) + 
  geom_errorbar(aes(x = train_size, 
                    ymin = mean.acc - z * sd.acc,
                    ymax = mean.acc + z * sd.acc, 
                    colour = 'not edited')) + 
  geom_errorbar(aes(x = train_size, 
                    ymin = mean.acc.edited - z * sd.acc.edited, 
                    ymax = mean.acc.edited + z * sd.acc.edited, 
                    colour = 'edited')) + 
  labs(x = 'training set size per class', y = 'test accuracy', 
       colour = NULL) + 
  theme(legend.key.width = unit(1, 'mm'),
        text = element_text(size = 9),
        legend.position = c(.85, .25),
        legend.background = element_blank())
```

## Clustering

The motivations behind clustering the training data embeddings are twofold: 
First, by drawing data uniformly across each cluster, we can draw samples that 
are relatively dissimilar from each other while reducing the chances of drawing
outliers compared to the diverse sampling method. Second, clustering may help 
us discover subclasses, and drawing balanced samples across these subclasses 
may result in better model performance (also suggested by 
\citet{NIPS2018_7396}). For each class, we used $k$-means 
clustering with $k = 5$ and drew training data maintaining cluster balance for 
each class. This was compared against random sampling (without using any 
cluster information). Our results suggest that these training subset selection 
methods result in equivalent model performance

```{r cifar10_clustered, fig.cap = 'Cluster-based sampling experiment on CIFAR-10. The $x$-axis is on the log-scale. Errorbars indicate $\\pm 1$ standard deviation from the mean of 10 repetitions.'}
cifar10.clustered.df %>% 
  dplyr::filter(train_set_type != 'diverse by cluster') %>% 
  dplyr::mutate(train_set_type = ifelse(train_set_type == 'random', 
                                        'random', 'by cluster')) %>% 
  dplyr::group_by(train_size, train_set_type) %>% 
  dplyr::summarise(mean.acc = mean(accuracy),
                   sd.acc = sd(accuracy)) %>% 
  ggplot() + 
  scale_x_log10() +
  # scale_y_log10() + 
  geom_line(aes(x = train_size, y = mean.acc, colour = train_set_type)) + 
  scale_colour_brewer(palette = 'Set1') + 
  geom_errorbar(aes(x = train_size, 
                    ymin = mean.acc - z * sd.acc, 
                    ymax = mean.acc + z * sd.acc, 
                    colour = train_set_type)) + 
  labs(x = 'training set size per class', 
       y = 'test accuracy',
       colour = NULL) + 
  theme(legend.key.width = unit(1, 'mm'),
        text = element_text(size = 9),
        legend.position = c(.8, .25),
        legend.background = element_blank())
```

## Transfer Learning

GIST and GIST-based embeddings have been used to fit high-performing image 
classification models \cite{Torralba2003b} 
\cite{Douze:2009:EGD:1646396.1646421}. In our experiments on the CIFAR-10
dataset, we found that a simple $5$-nearest neighbors model achieves accuracy 
that is significantly above that of using pixel-level features despite a 
threefold reduction in dimensionality[^dims]. But an alternate embedding may 
more accurately describe image similarity/dissimilarity for a particular 
dataset. In this section, we will try using a "best case scenario" embedding 
short of actually fitting a model to the CIFAR-10 data. 

One view of CNNs is as a supervised image embedding algorithm. The second to 
last layer of an image classification CNN assigns each image a point in 
$\mathbb{R}^q$ where $q$ is the number of nodes of the layer, then the last 
layer performs multinomial logistic regression to assign a predicted label to 
each point in that space. A perfectly accurate CNN model will then produce an 
embedding such that the classes are linearly separable. Using this as our 
motivation, we constructed an embedding of CIFAR-10 images based on the Inception V3 \cite{43022} network that has been pre-trained[^keras] on the ImageNet \cite{imagenet_cvpr09} dataset. The Inception V3 embedding was then used to draw training subsamples. This is not a realistic use-case as the CIFAR-10 labels are a subset of the ImageNet labels, but this hopefully serves as an "oracle" embedding[^inception].

[^dims]: GIST features are in $\mathbb{R}^{960}$ while pixel features for CIFAR-10  are in $\mathbb{R}^{3072}$.

[^keras]: Pretrained model provided by the Keras \cite{chollet2015keras} 
package.

[^inception]: A cross-validated $5$-nearest neighbors model using this 
embedding results in approximately 80\% accuracy, compared to approximately 
60\% accuracy using the GIST embedding.

The subsampling methods used here are:

1. Diverse sampling based on maximal interpoint distances
2. Uniform random sampling
3. Wilson editing followed by diverse sampling
4. Stratified by cluster (based on $5$-means clustering)
5. Wilson editing followed by cluster-stratified sampling

```{r cifar10_transfer, fig.cap = 'Results from various experiments on drawing training images using the Inception V3 embedding. The $x$-axis is on the log-scale. Errorbars indicate $\\pm 1$ standard deviation from the mean of 10 repetitions.'}
cifar10.translearn.df %>% 
  dplyr::filter(train_set_type != 'similar') %>%
  dplyr::group_by(train_size, train_set_type) %>% 
  dplyr::summarise(mean.acc = mean(accuracy),
                   sd.acc = sd(accuracy)) %>% 
  dplyr::mutate(
    train_set_type = factor(train_set_type, 
                            levels = c('diverse', 
                                       'random',
                                       'diverse with editing', 
                                       'by cluster', 
                                       'edited and by cluster'))) %>% 
  ggplot() + 
  scale_x_log10() +
  # scale_y_log10() + 
  geom_line(aes(x = train_size, y = mean.acc, colour = train_set_type)) + 
  scale_colour_brewer(palette = 'Set1') + 
  geom_errorbar(aes(x = train_size, 
                    ymin = mean.acc - z * sd.acc, 
                    ymax = mean.acc + z * sd.acc, 
                    colour = train_set_type)) + 
  labs(x = 'training set size per class', 
       y = 'test accuracy',
       colour = NULL) + 
  theme(legend.key.width = unit(1, 'mm'),
        text = element_text(size = 9),
        legend.position = c(.85, .3),
        legend.background = element_blank())
```

```{r embed_compare, fig.cap = 'Comparison of sampling CIFAR-10 images using GIST and Inception V3 embeddings. The $x$-axis is on the log-scale. Errorbars indicate $\\pm 1$ standard deviation from the mean of 10 repetitions.'}
dplyr::bind_rows(
  cifar10.translearn.df %>% 
    dplyr::filter(train_set_type != 'similar') %>%
    dplyr::group_by(train_size, train_set_type) %>% 
    dplyr::summarise(mean.acc = mean(accuracy),
                     sd.acc = sd(accuracy)) %>% 
  dplyr::mutate(embedding = 'inception v3'),
  cifar10.val.from.train.df %>% 
    dplyr::bind_rows(
      cifar10.edited.df %>%
        dplyr::filter(train_set_type %in% c('diverse', 'random')) %>%
        dplyr::mutate(train_set_type = ifelse(train_set_type == 'random', 
                                              'random', 
                                              'diverse with editing'))) %>%
    dplyr::bind_rows(
      cifar10.clustered.df %>%
        dplyr::filter(train_set_type %in% c('random by cluster', 'random')) %>%
        dplyr::mutate(train_set_type = ifelse(train_set_type == 'random', 
                                              'random', 'by cluster'))) %>%
    dplyr::bind_rows(
      cifar10.translearn.df %>% 
        dplyr::filter(train_set_type %in% c('random by cluster', 'random')) %>% 
        dplyr::mutate(
          train_set_type = ifelse(train_set_type == 'random', 
                                  'random', 'transfer learning'))) %>%
    dplyr::group_by(train_size, train_set_type) %>%
    dplyr::summarise(mean.acc = mean(accuracy),
                     sd.acc = sd(accuracy)) %>% 
    dplyr::mutate(embedding = 'gist')
) %>%
  dplyr::filter(train_size %in% c(10, 20, 40, 60, 80, 
                                  100, 200, 250, 500)) %>% 
  dplyr::filter(train_set_type != 'similar') %>% 
  dplyr::filter(train_set_type != 'edited and by cluster') %>% 
  ggplot() + 
  scale_x_log10() +
  # scale_y_log10() + 
  geom_line(aes(x = train_size, y = mean.acc, 
                # colour = train_set_type,
                linetype = embedding)) + 
  scale_colour_brewer(palette = 'Set1') + 
  geom_errorbar(aes(x = train_size, 
                    ymin = mean.acc - z * sd.acc, 
                    ymax = mean.acc + z * sd.acc, 
                    # colour = train_set_type,
                    linetype = embedding),
                width = .05) + 
  labs(x = 'training set size per class', 
       y = 'test accuracy',
       colour = NULL) + 
  theme(legend.key.width = unit(3, 'mm'),
        text = element_text(size = 9),
        # legend.position = c(.85, .25),
        legend.background = element_blank()) + 
  facet_wrap(~ train_set_type)
```

The results are largely consistent with those from using GIST embeddings. 
For a large enough training subsample, the methods converge to the same test 
accuracy, and for smaller subsamples, random and cluster-based sampling results 
in equivalent performance while diverse sampling results in reduced 
performance (Fig. \ref{fig:cifar10_transfer}). Our results further suggest that 
there is no significant difference between using the GIST embedding or the 
Inception V3 embedding for subsampling training data (Fig. \ref{fig:embed_compare}).

# Conclusions and Future Work

In our work, we attempted to replicate the results of \citet{NIPS2018_7396} on 
separate datasets. Then we tried to expand upon their methods via data editing 
and cluster-stratified sampling. These methods were compared against diverse 
and similar sampling methods based on GIST embeddings as well as uniform random 
sampling. 

We failed to find a sampling method for images that outperforms uniform random 
sampling. In fact, our attempt at drawing a "diverse" set of images results in 
reduced performance. We further demonstrated that Wilson data editing on the
training set may improve performance and cluster-stratified sampling is on par 
with random sampling. As expected, "similar" training subsets resulted in
significantly poorer performance than the other methods (Fig.
\ref{fig:compare}).

```{r compare, fig.width = 6, fig.height = 4, fig.cap = 'Comparison of the various image sampling methods and the resulting model performance on the CIFAR-10 test set.'}
cifar10.val.from.train.df %>% 
  dplyr::bind_rows(
    cifar10.edited.df %>%
      dplyr::filter(train_set_type %in% c('diverse', 'random')) %>%
      dplyr::mutate(train_set_type = ifelse(train_set_type == 'random', 
                                            'random', 
                                            'diverse with editing'))) %>%
  dplyr::bind_rows(
    cifar10.clustered.df %>%
      dplyr::filter(train_set_type %in% c('random by cluster', 'random')) %>%
      dplyr::mutate(train_set_type = ifelse(train_set_type == 'random', 
                                            'random', 'by cluster'))) %>%
  dplyr::bind_rows(
    cifar10.translearn.df %>% 
      dplyr::filter(train_set_type %in% c('random by cluster', 'random')) %>% 
      dplyr::mutate(train_set_type = ifelse(train_set_type == 'random', 
                                            'random', 'transfer learning'))) %>%
  dplyr::group_by(train_size, train_set_type) %>%
  dplyr::summarise(mean.acc = mean(accuracy),
                   sd.acc = sd(accuracy)) %>%
  dplyr::filter(train_size %in% c(10, 20, 40, 60, 80, 
                                  100, 200, 250, 500)) %>% 
  dplyr::mutate(train_set_type = factor(train_set_type, 
                                        levels = c('diverse', 
                                                   'similar',
                                                   'random',
                                                   'diverse with editing', 
                                                   'by cluster',
                                                   'transfer learning'))) %>% 
  ggplot() + 
  scale_x_log10() +
  # scale_y_log10() + 
  geom_line(aes(x = train_size, y = mean.acc, colour = train_set_type)) + 
  scale_colour_brewer(palette = 'Set1') + 
  geom_errorbar(aes(x = train_size, 
                    ymin = mean.acc - z * sd.acc, 
                    ymax = mean.acc + z * sd.acc, 
                    colour = train_set_type,
                    fill = train_set_type),
                width = .05) + 
  labs(x = 'training set size per class', 
       y = 'test accuracy',
       colour = NULL) + 
  theme(legend.key.width = unit(1, 'mm'),
        text = element_text(size = 9),
        legend.position = c(.85, .25),
        legend.background = element_blank())
```

\citet{NIPS2018_7396} suggested clustering the embedded images and drawing 
representative samples from each cluster. Our results show that 
cluster-straified sampling is not significantly better than random sampling, 
but we only tried one type of clustering and simply drew cluster-stratified 
samples. Future work may involve more sophisticated clustering techniques and 
identifying images that are representative or characteristic of each cluster.

Based on work by \citet{wang2018data} and \citet{kaushal2019learning} as well 
as our embedding-based approaches, future attempts may involve active learning
based methods using iterative CNN-generated embeddings. 

```{r one_col, results='asis'}
# cat('\\onecolumn')
```

# References